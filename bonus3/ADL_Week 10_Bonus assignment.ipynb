{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aAI1FURLjt4X",
   "metadata": {
    "id": "aAI1FURLjt4X"
   },
   "source": [
    "# Creating a recommender system with RNNs\n",
    "\n",
    "We will observe the history of items that customers have bought from our online store. Our objective is to predict the next item that a customer will buy, given their purchase history.\n",
    "\n",
    "**Question 1**: Assuming you can predict well what customers are going to buy when visiting our store. What can you do with this information in order to improve the profits of our online store?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9b7631",
   "metadata": {
    "id": "6c9b7631"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengjunming/Desktop/Anaconda/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/Users/pengjunming/Desktop/Anaconda/anaconda3/envs/myenv/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, BatchNormalization, GRU, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hv2XW-dRjzki",
   "metadata": {
    "id": "hv2XW-dRjzki"
   },
   "source": [
    "## 1. The data\n",
    "\n",
    "Each entry in the dataset corresponds to a combination of customer and item bought. Per customer, items are arranged based on the visits to our online store (e.g., customer 17850 first bought item 0, then item 399, then item 505, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdf310c",
   "metadata": {
    "id": "9cdf310c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>StockCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  StockCode\n",
       "0     17850.0          0\n",
       "1     17850.0        399\n",
       "2     17850.0        505\n",
       "3     17850.0        227\n",
       "4     17850.0        253"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://www.dropbox.com/s/4kicl5okwlmst5i/online_retail.csv?dl=1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZZvSj9QfbinS",
   "metadata": {
    "id": "ZZvSj9QfbinS"
   },
   "source": [
    "We will later use zero-padding to get sequences of equal length. Hence, we should avoid items with name \"0\" and instead shift all items by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddPc8BCH9yV",
   "metadata": {
    "id": "eddPc8BCH9yV"
   },
   "outputs": [],
   "source": [
    "df['StockCode'] = df['StockCode'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4970c79c",
   "metadata": {
    "id": "4970c79c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_items = len(df['StockCode'].unique())\n",
    "number_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WeKafihlk0UI",
   "metadata": {
    "id": "WeKafihlk0UI"
   },
   "source": [
    "We convert the dataframe into a list of sequences, where each customer corresponds to one sequence of items bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308a0b3b",
   "metadata": {
    "id": "308a0b3b"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for customer in df['CustomerID'].unique():\n",
    "    temp = df[df['CustomerID'] == customer]\n",
    "    sequences.append(temp['StockCode'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K8mdeMoMk9YQ",
   "metadata": {
    "id": "K8mdeMoMk9YQ"
   },
   "source": [
    "Some sequences are much longer than others, so we will only consider sequences of a certain length. In particular, we pick here approximately the 90% quantile to cut off sequences of purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ee9767",
   "metadata": {
    "id": "c5ee9767"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile([len(seq) for seq in sequences],0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95d8678",
   "metadata": {
    "id": "f95d8678"
   },
   "outputs": [],
   "source": [
    "max_length = 160\n",
    "sequences = [seq[:min(max_length,len(seq))] for seq in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0599HYqlH1B",
   "metadata": {
    "id": "d0599HYqlH1B"
   },
   "source": [
    "We also add \"padding\" to make sequences of equal length (to train our model, each sequence within a mini-batch has to have the same length. Since we don't want to have a lot of work splitting the data into mini-batches, we will just equalize everything). Note that we will need to tell our algorithm later to ignore padded values when it comes to loss-computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b11131",
   "metadata": {
    "id": "42b11131"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102, 124, 367, 12, 16, 46, 500, 177, 188, 231]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7262f9",
   "metadata": {
    "id": "fe7262f9"
   },
   "outputs": [],
   "source": [
    "sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07867de",
   "metadata": {
    "id": "b07867de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 102, 124, 367,  12,  16,  46,\n",
       "       500, 177, 188, 231], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cyrgoaqm3en",
   "metadata": {
    "id": "5cyrgoaqm3en"
   },
   "source": [
    "Next, we split the data into training, validation, and testing (randomly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EZUe69DeT2sN",
   "metadata": {
    "id": "EZUe69DeT2sN"
   },
   "outputs": [],
   "source": [
    "X_train, X_other = train_test_split(sequences,train_size=0.8)\n",
    "X_valid, X_test = train_test_split(X_other,train_size=0.5)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d3309",
   "metadata": {},
   "source": [
    "Finally, we build our target (y) based on a sequence-to-sequence approach. That is, for each sequence of inputs, we predict a sequence of outputs.\n",
    "\n",
    "**Question 2**: Construct the variables `y_train`, `y-valid`, and `y_test`. Then, modify `X_train`, `X_valid`, and `X_test` appropriately to ensure that input and output sequences are of the same length. *Hint*: After any item has been bought, we want to predict the next item bought by the same customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9624434",
   "metadata": {
    "id": "b9624434"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b25531",
   "metadata": {},
   "source": [
    "Check your sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292bb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M7FsxQEHm_xx",
   "metadata": {
    "id": "M7FsxQEHm_xx"
   },
   "source": [
    "## 2. Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183da97",
   "metadata": {},
   "source": [
    "We now build a model that takes as input a sequence of orders by one customer and outputs the predictions for the next time step. Instead of directly using our sequences as inputs to a recurrent layer, we will use an `Embedding` layer.\n",
    "\n",
    "**Question 3**: In your own words, describe what (word) embeddings do, and why we use them in deep learning. A good resource is the accompanying book \"Deep Learning with Python\" (2nd edition) by Francois Chollet, available online through the City University Library. You might want to check the part \"Understanding Word Embeddings\" within Chapter 11.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QzAk-wbMbz9z",
   "metadata": {
    "id": "QzAk-wbMbz9z"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Embedding(input_dim=number_items+1, output_dim=6, input_shape=[None], mask_zero=True),\n",
    "    Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    GRU(64, return_sequences=True, dropout = 0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(number_items+1, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jeiqtGhXLPg3",
   "metadata": {
    "id": "jeiqtGhXLPg3"
   },
   "source": [
    "We want to add our own metric, to capture how well we're doing on the last prediction (that's the only one that matters after all). In particular, we will see whether the product the customer bought is within the 5 products we gave the highest probability in our prediction.\n",
    "\n",
    "**Question 4**: Define a function `last_time_step_top_5` that takes the inputs `y_true` and `y_pred` and computes the `tf.keras.metrics.sparse_top_k_categorical_accuracy` between `y_true` and `y_pred` *for the last entry of each sequence*. Note that `sparse_top_k_categorical_accuracy` takes as input the (modified) `y_true` and `y_pred`, as well as a value `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mtL6xq4o-2r_",
   "metadata": {
    "id": "mtL6xq4o-2r_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JlVSMayCLVn5",
   "metadata": {
    "id": "JlVSMayCLVn5"
   },
   "source": [
    "We are now ready to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DmVSCZw5_AgL",
   "metadata": {
    "id": "DmVSCZw5_AgL"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                metrics = [last_time_step_top_5])\n",
    "log = model.fit(X_train, y_train, epochs=20,\n",
    "                validation_data = (X_valid,y_valid))\n",
    "\n",
    "plt.plot(log.history['last_time_step_top_5'],label = \"actual in top 5 - training\",color='green')\n",
    "plt.plot(log.history['val_last_time_step_top_5'], label = \"actual in top 5 - validation\",color='grey')\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cKiB9jncPLFr",
   "metadata": {
    "id": "cKiB9jncPLFr"
   },
   "source": [
    "On the one hand, this doesn't sound too impressive. On the other hand, keep in mind that we have looked at raw items, and 1000 of them (while only having the buying history of 3500 customers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4e875",
   "metadata": {
    "id": "HTjV92hnxpPg"
   },
   "source": [
    "**Question 5**: Can you do better? Go through the frameworks we have discussed in class in order to generate an improved model. A few hints:\n",
    "- Before thinking about our framework for improving bias and variance, note that the model does not yet really overfit\n",
    "- While we generally don't stack recurrent layers too deeply for computational reasons, we are currently only using a single one\n",
    "- Consider the specific type of dropout regularization relevant for RNNs\n",
    "- Aside from the typical suspects for parameters to modify, the number of dimensions of the embedding usually has a big influence\n",
    "\n",
    "At the end of your improvement process, evaluate your model on the test set."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ADL_Week 10_Recommender RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
