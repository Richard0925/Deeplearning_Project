{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d80bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, UpSampling2D, MaxPool2D, Flatten, Dense, BatchNormalization, SeparableConv2D, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0f775",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We'll be using the [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), which contains some natural images tagged in 10 different categories (e.g. cars, dogs, birds etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d553ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # number of classes in the data\n",
    "img_rows, img_cols, img_channels = 32, 32, 3 # input image dimensions\n",
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "# Load and convert data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Convert class vectors to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, img_channels)\n",
    "\n",
    "# To speed up things (and be closer to typical problems in practice) we select 1k random samples for training and test\n",
    "index = np.arange(x_train.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(index)\n",
    "index = index[:1000]\n",
    "x_train, y_train = x_train[index], y_train[index]\n",
    "\n",
    "index = np.arange(x_test.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(index)\n",
    "index = index[:1000]\n",
    "x_test, y_test = x_test[index], y_test[index]\n",
    "\n",
    "# We will also normalize the data\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc1be9",
   "metadata": {},
   "source": [
    "Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b508ab",
   "metadata": {},
   "source": [
    "# 1. From the Sequential API to the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81e86c",
   "metadata": {},
   "source": [
    "## 1.1 An initial CNN (the usual way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176153d",
   "metadata": {},
   "source": [
    "Let's start by building and training a CNN from scratch, as we have seen many times before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f79fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape = input_shape),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=128, kernel_size=3, activation=\"relu\"),\n",
    "    Flatten(),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af884be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7ac1d",
   "metadata": {},
   "source": [
    "## 1.2 The same, but different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fdfd26",
   "metadata": {},
   "source": [
    "We will now repeat the process. However, instead of using TensorFlow's Sequential API, we will now use the Functional API. What's the difference? Essentially, each layer is now defined independently of the others. We are connecting layers by using previous layers as their inputs. Because layers are independent, we have to specify a `tf.keras.Model` with all the inputs and outputs of our model.\n",
    "\n",
    "Let's take a look, by creating the same model as before using the Functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59a833",
   "metadata": {},
   "source": [
    "We train this model in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3d14d",
   "metadata": {},
   "source": [
    "## 1.3 A simple non-sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be42ea",
   "metadata": {},
   "source": [
    "So far, there was no difference. The Functional API is more powerful, however. In particular, it allows us to train non-sequential models (that is, models that are not simply a stack of one layer after the other.\n",
    "\n",
    "Here, we will train a model that predicts both the class of an images, as well as a random number (this doesn't have much meaning but it is really just to show you how we can use the Functional API of TensorFlow).\n",
    "\n",
    "Let's first create the secondary y's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482259e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = np.mean(x_train,axis=(1,2,3))\n",
    "y2_train = np.random.normal(train_means,np.abs(train_means/2))\n",
    "\n",
    "test_means = np.mean(x_test,axis=(1,2,3))\n",
    "y2_test = np.random.normal(test_means,np.abs(test_means/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ae0ae",
   "metadata": {},
   "source": [
    "Let's now build the model. Note, in particular, that we have two outputs, and each takes as their input the `Flatten` layer. When we create the model, we simply specify `outputs` using a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b062f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "output1 = Dense(num_classes, activation=\"softmax\", name = 'output_1')(x)\n",
    "output2 = Dense(1, activation=\"sigmoid\", name = 'output_2')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[output1,output2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092fbb8",
   "metadata": {},
   "source": [
    "Plotting the model, we directly see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c81a1",
   "metadata": {},
   "source": [
    "Let's now compile our non-sequential model. We need to define our losses and metrics for each of our outputs! This is why we gave the output layers specific names, so we can use this here. Alternatively, you could also specify these as a list, so you don't necessarily need to have a name for each.\n",
    "\n",
    "Note that we also add `loss_weights`, to define how important each part of the outputs (or the loss on those) is when assessing the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c22928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss={'output_1':'binary_crossentropy',\n",
    "                         'output_2':'mean_squared_error'},\n",
    "               loss_weights = [1,0.01],\n",
    "               metrics = {'output_1':'accuracy',\n",
    "                         'output_2':'mean_squared_error'},\n",
    "               optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52284b9",
   "metadata": {},
   "source": [
    "We train the model as before. Because we have two outputs, we also have to have two y-variables. Again, we can simply use a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa484a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y=[y_train,y2_train],\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddc4c4",
   "metadata": {},
   "source": [
    "Similarly, when we evaluate the model, we need to specify multiple y's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b975a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, [y_test,y2_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1965906",
   "metadata": {},
   "source": [
    "And we can make predictions as before. We just have to note that two outputs are being predicted, the labels and the average values. But we can simply use list-indices to get the right ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(predictions[0].shape)\n",
    "print(predictions[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb64fda",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "- What key differences do you observe between the Sequential API and the Functional API?\n",
    "- For which types of applications is the Sequential API insufficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4474d6",
   "metadata": {},
   "source": [
    "## 1.4 A typical application for non-sequential models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03eb7f",
   "metadata": {},
   "source": [
    "Non-sequential often come in handy and most people really use the Functional API as the default due to its greater flexibility. One application area is object detection.\n",
    "\n",
    "**Discussion**: Why do we need multiple outputs when doing object detection?\n",
    "\n",
    "While building an object detction model from scratch is very time-consuming, let's take a look at a pre-built one. In particular, we will use this step to introduce the TensorFlow Hub, where you can download a large range of models. You might have to first install the corresponding package. On the website of the [TensorFlow Hub](https://tfhub.dev/) you can find models based on the application you are interested in. Take a look, for example, at [models for object detection](https://tfhub.dev/s?module-type=image-object-detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405617cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d97a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f71e90",
   "metadata": {},
   "source": [
    "We will load a pre-trained model \"Efficient Det\". Note that this is not a Keras model, but a pure TensorFlow model, so we cannot do things like displaying a summary (but we can use the model to run predictions). Don't worry about the long loading time and the warnings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5678590",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_model = hub.load('https://tfhub.dev/tensorflow/efficientdet/d0/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd7d2c",
   "metadata": {},
   "source": [
    "Our model takes images and creates classifications for different objects within the images (together with bounding boxes). Each class has a number, so we need to be able to go from that number to the actual class name. The following pieces of code load the '.pbtxt' file necessary for this transformation, and creates a dictionary mapping class numbers to names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a58d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_map(label_map_path):\n",
    "    item_id = None\n",
    "    item_name = None\n",
    "    items = {}\n",
    "    with open(label_map_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line.replace(\" \", \"\")\n",
    "            if line == \"item{\":\n",
    "                pass\n",
    "            elif line == \"}\":\n",
    "                pass\n",
    "            elif \"id\" in line:\n",
    "                item_id = int(line.split(\":\", 1)[1].strip())\n",
    "            elif \"display_name\" in line:\n",
    "                item_name = line.split(\":\", 1)[1].replace('\"', '').strip()\n",
    "\n",
    "            if item_id is not None and item_name is not None:\n",
    "                items[item_id] = item_name\n",
    "                item_id = None\n",
    "                item_name = None\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706597ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/mscoco_label_map.pbtxt\")\n",
    "f = open(\"mscoco_label_map.pbtxt\",'wb')\n",
    "f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e414f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = read_label_map(\"mscoco_label_map.pbtxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bdd0db",
   "metadata": {},
   "source": [
    "If you explore `class_names`, you will get an idea of the kind of categories our model can detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab654b",
   "metadata": {},
   "source": [
    "We now load and display an example image, which we will run through our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d18d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/1/1f/A_street_scene_in_Coimbatore%2C_Tamil_Nadu%2C_India.jpg\"\n",
    "url_response = urllib.request.urlopen(url)\n",
    "img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
    "img = cv2.imdecode(img_array, -1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "h, w, _ = img.shape\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc527f",
   "metadata": {},
   "source": [
    "We now apply our model to that image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = np.expand_dims(img, 0)\n",
    "res = hub_model(input_tensor)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c1aea",
   "metadata": {},
   "source": [
    "There are 8 different outputs from our object detection model. Luckily, the outputs are in a dictionary with quite interpretable names. Take a look.\n",
    "\n",
    "**Discussion**: can you interpret what types of outputs we have here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb849911",
   "metadata": {},
   "source": [
    "We can now display our bounding boxes and classifications on top of the image. We are not displaying all bounding boxes, but only those for which our model is relatively certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imdecode(img_array, -1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "for boxes, classes, scores in zip(res['detection_boxes'].numpy(), res['detection_classes'], res['detection_scores'].numpy()):\n",
    "    for box, cls, score in zip(boxes, classes, scores): # iterate over sub values in list\n",
    "        if score > 0.5: # our confidence threshold for displaying boxes\n",
    "            ymin = int(box[0] * h)\n",
    "            xmin = int(box[1] * w)\n",
    "            ymax = int(box[2] * h)\n",
    "            xmax = int(box[3] * w)\n",
    "            # write classname for bounding box\n",
    "            cv2.putText(img, class_names[tf.cast(cls,dtype=tf.uint8).numpy()], (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (238, 160, 0), 4)\n",
    "            # draw on image\n",
    "            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (238, 160, 0), 4)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce7fb6",
   "metadata": {},
   "source": [
    "**Discussion**:  Changing the confidence threshold, what do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60256e5b",
   "metadata": {},
   "source": [
    "# 2. Some practical issues specific to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c749e",
   "metadata": {},
   "source": [
    "## 2.1 Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43b5f1",
   "metadata": {},
   "source": [
    "The general idea is that we add the input of a block to the output, in order to avoid loss of information. This is accomplished with `tf.keras.layers.add([x,residual])`. However, there is a problem, as you will discover when running the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "residual = inputs\n",
    "\n",
    "x = tf.keras.layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f18e",
   "metadata": {},
   "source": [
    "We have to ensure that our residual has the same dimensions. In this case, we need more channels and we also need to make sure that our block doens't reduce the size. Take a look at the following piece of code.\n",
    "\n",
    "**Discussion**: In the next code block, can you spot how we avoid the previous issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceca232",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "residual = inputs\n",
    "residual = Conv2D(32, 1)(residual)\n",
    "\n",
    "x = tf.keras.layers.add([x, residual])\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef56d23",
   "metadata": {},
   "source": [
    "Let's scale this up. We want to create a number of neural network with three blocks, as before (32 filters, 64 filters, 128 filters). However, each block also contains a `MaxPooling` layer, while the residual is used to connect the input to the block to the output. As in the original example, however, we avoid the `MaxPooling` layer at the last block.\n",
    "\n",
    "**Discussion**: Why do we need apply `strides=2` for the convolution of the residual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4374a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = inputs\n",
    "\n",
    "def residual_block(x, filters,pooling=False):\n",
    "    residual = x\n",
    "    x = Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    if pooling:\n",
    "        x = MaxPooling2D(2, padding=\"same\")(x)\n",
    "        residual = Conv2D(filters, 1, strides=2)(residual) \n",
    "    else:\n",
    "        residual = Conv2D(filters, 1)(residual)\n",
    "    x = tf.keras.layers.add([x, residual])\n",
    "    return x\n",
    "\n",
    "x = residual_block(x, filters=32,pooling=True)\n",
    "x = residual_block(x, filters=64,pooling=True)\n",
    "x = residual_block(x, filters=128,pooling=False)\n",
    "\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d96f97",
   "metadata": {},
   "source": [
    "## 2.2 Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8ab5d",
   "metadata": {},
   "source": [
    "We have seen Data augmentation before, using the ImageDataGenerator. Starting with TensorFlow version 2.2, there are also layers that can be added into the network that essentially do the same thing. They have one key advantage: the augmentation process happens on the GPU rather than the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "x = tf.keras.layers.RandomFlip(\"horizontal\") (inputs)\n",
    "x = tf.keras.layers.RandomRotation(0.1)(x)\n",
    "x = tf.keras.layers.RandomZoom(0.2)(x)\n",
    "\n",
    "x = residual_block(x, filters=32,pooling=True)\n",
    "x = residual_block(x, filters=64,pooling=True)\n",
    "x = residual_block(x, filters=128,pooling=False)\n",
    "\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de972f0",
   "metadata": {},
   "source": [
    "## 2.3 Depthwise separable convolutions and putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ff749",
   "metadata": {},
   "source": [
    "We combine some of the architectural principles that we learned about to build a simplified version of the [Xception](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "x = tf.keras.layers.RandomFlip(\"horizontal\")(inputs)\n",
    "x = tf.keras.layers.RandomRotation(0.1)(x)\n",
    "x = tf.keras.layers.RandomZoom(0.2)(x)\n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32, 64, 128]:\n",
    "    residual = x\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = tf.keras.layers.add([x, residual])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ebb7b",
   "metadata": {},
   "source": [
    "Note that this is quite the big model for the amount of data we have, so we need to put in a lot more work to train. The below code will get you only so far. For better performance, you will want to fine-tune the learning rate and batch size and add a learning rate schedule (and increase the number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d91479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=40,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f37ec5",
   "metadata": {},
   "source": [
    "**Discussion**: can you fine-tune the process in order to get a better-performing model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9caf6f",
   "metadata": {},
   "source": [
    "# 3. Transfer learning to use pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c55828",
   "metadata": {},
   "source": [
    "We will now use a pre-trained Xception network instead of trying to train one from scratch. There are multiple ways to do so. The first will rely on using only the outputs from the network and training our own classifier based on those:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24be425",
   "metadata": {},
   "source": [
    "## 3.1 Using pre-trained model outputs as inputs to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5e870",
   "metadata": {},
   "source": [
    "In order to run the Xception network that comes directly with TensorFlow, we need to adjust our data. Luckily most of the hard lifting is done by the `preprocess_input` function that comes with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbd22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb32980",
   "metadata": {},
   "source": [
    "Note that the function takes numpy arrays with pixel values from 0-255. As we had normalized earlier, we need to rescale first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_preprocess_train = preprocess_input(x_train.copy()*255)\n",
    "x_preprocess_test = preprocess_input(x_test.copy()*255)\n",
    "print(x_preprocess_train.shape)\n",
    "print(x_preprocess_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda3ae1",
   "metadata": {},
   "source": [
    "As you can find in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/Xception), we need to have images with that are at least 71x71. Say you have smaller images (in our case 32x32) and we want to make them bigger. There are different ways to do this. One way is to use the `zoom` from `scipy`. This adds in new pixels whose value is based on spline interpolation. Run the following code, which might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffaf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "print('Processing training data')\n",
    "resized_images = []\n",
    "for i in range(x_preprocess_train.shape[0]):\n",
    "    resized_images.append(zoom(x_preprocess_train[i], (5.0, 5.0, 1.0)))\n",
    "    if (i+1) % 200 == 0:\n",
    "        print('* Processed %d images' % (i+1))\n",
    "x_preprocess_train = np.stack(resized_images)\n",
    "\n",
    "print('Processing testing data')\n",
    "resized_images = []\n",
    "for i in range(x_preprocess_test.shape[0]):\n",
    "    resized_images.append(zoom(x_preprocess_test[i], (5.0, 5.0, 1.0)))\n",
    "    if (i+1) % 200 == 0:\n",
    "        print('* Processed %d images' % (i+1))\n",
    "x_preprocess_test = np.stack(resized_images)\n",
    "\n",
    "print(x_preprocess_train.shape)\n",
    "print(x_preprocess_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ccd03",
   "metadata": {},
   "source": [
    "Let's plot again our sample image (which needs to be reshaped appropriately again): You will see that the resolution is much better, because we now have more pixels. How did we decide to fill those pixels up? By interpolating between the values of the pixels around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7938b",
   "metadata": {},
   "source": [
    "Let's plot again our sample image (which needs to be reshaped appropriately again): You will see that the resolution is much better, because we now have more pixels. How did we decide to fill those pixels up? By interpolating between the values of the pixels around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(x_preprocess_train[0] + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b4db3",
   "metadata": {},
   "source": [
    "We are now ready to build a model. Our basis will be the Xception network, with the parameters trained on ImageNet.  Notice we add the `include_top=False` parameter because we don't want to use the included ImageNet classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xception = Xception(weights='imagenet', include_top=False, input_shape = (160,160,3))\n",
    "xception.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65061dc9",
   "metadata": {},
   "source": [
    "Xception outputs a (5, 5, 2048) tensor before it's \"top layer\" (which we didn't download).  We compute the relevant activation of this top layer for our images (this represents a range of high-level features of our images). We then aggregate the top layer activation using a `GlobalAveragePooling2D` layer, which compresses it into a (1, 1, 2048) tensor, which we can easily turn into a 2048-vector.  `GlobalAveragePooling2D` works by taking the average of each 5x5 feature map.\n",
    "\n",
    "Let's extract high-level features of our images using Xception (this may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features_train = xception.predict(x_preprocess_train)\n",
    "x_features_test = xception.predict(x_preprocess_test)\n",
    "print(x_features_train.shape)\n",
    "print(x_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae9e07",
   "metadata": {},
   "source": [
    "Let's now build a simple feed-forward network which uses these features as an input, to predict the right classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    GlobalAveragePooling2D(input_shape=(5,5,2048)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_features_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285f99",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "1. What happens if you don't use the `GlobalAveragePooling2D` layer (note: you will need a `Flatten` layer instead)?  Hypothesize why the performance changes.\n",
    "1. Why does the image need to be preprocessed via the `preprocess_input()` function?  Hypothesize what happens if we omit this step.\n",
    "1. Why do we need to resize the image (via `zoom`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a747ecd",
   "metadata": {},
   "source": [
    "## 3.2 Using Frozen Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4db3ac",
   "metadata": {},
   "source": [
    "Instead of using pre-processed features, we add the pre-trained MobileNet network to our model (and freeze it, to avoid changing its weights). This allows us to do things like data augmentation, or to process our data on the go. We start from the (non-processed data)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_preprocess_train = preprocess_input(x_train.copy()*255)\n",
    "x_preprocess_test = preprocess_input(x_test.copy()*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44495a",
   "metadata": {},
   "source": [
    "We integrate the `Xception` layers into our network. But before that, we need to resize the images. This time, we use \"upsampling\". Basically, we add additional pixels that are copies of the existing ones, i.e., we \"stretch out\" the pixels.\n",
    "\n",
    "We will conduct upsampling directly in our neural network, so there is not need to do additional pre-processing. After the upsampling, we also do some data augmentation, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ec9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xception = Xception(weights='imagenet', include_top=False, input_shape = (160,160,3))\n",
    "xception.trainable = False # This is key! We don't want to change the weights of the pre-trained model (for now)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    UpSampling2D(size=(5,5), input_shape = input_shape),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    xception,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')])\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f7f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_preprocess_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd78da",
   "metadata": {},
   "source": [
    "**Discussion**:\n",
    "\n",
    "1. Why does the network train so much more slowly?\n",
    "1. What is the difference (if any) between this model and the previous one?\n",
    "1. How would you modify the above code if we wanted to train some of the Xception layers, but not all? Hint: you can iterate through the layers of a model with\n",
    "```\n",
    "for layer in model.layers:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93fb03",
   "metadata": {},
   "source": [
    "# 4. Hot-Dog or Not-Dog: apply what you learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590abc1",
   "metadata": {
    "id": "f590abc1"
   },
   "source": [
    "This problem's purpose is to build a neural network to classify images as hot dogs or not-hot dogs. This is the same problem as seen in the HBO TV show \"Silicon Valley\". We will be using the dataset put together by [a user on Kaggle](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog) which contains 498 training images and 500 test images.\n",
    "\n",
    "A simple CNN is given below. Due to the small sample size it has a very poor test set accuracy. Your task is to build a CNN that can beat this test set accuracy by a large margin (get to at least 70% test set accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8080ba5",
   "metadata": {
    "id": "d8080ba5"
   },
   "source": [
    "First, we need a few more packages. If you don't currently have skimage or cv2 installed, uncomment and run the lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48558e4",
   "metadata": {
    "id": "d48558e4"
   },
   "outputs": [],
   "source": [
    "#pip install scikit-image\n",
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4780a7b",
   "metadata": {
    "id": "d4780a7b"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import cv2\n",
    "import tarfile\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4107e4",
   "metadata": {
    "id": "3c4107e4"
   },
   "source": [
    "We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748420a0",
   "metadata": {
    "id": "748420a0"
   },
   "outputs": [],
   "source": [
    "path_to_data =  tf.keras.utils.get_file('hotdog.tar', 'https://www.dropbox.com/s/9zx61bhlrjx135j/hotdog.tar?dl=1')\n",
    "file = tarfile.open(path_to_data)\n",
    "file.extractall(os.path.abspath(os.path.join(path_to_data, os.pardir)))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226e774",
   "metadata": {
    "id": "b226e774"
   },
   "source": [
    "Let's take a look at two examples pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cacabd",
   "metadata": {
    "id": "54cacabd"
   },
   "outputs": [],
   "source": [
    "img_size = 160\n",
    "\n",
    "img_arr_hotdog = cv2.imread(os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/train/hot_dog/2417.jpg')))\n",
    "img_arr_hotdog = cv2.resize(img_arr_hotdog, (img_size,img_size))[:,:,::-1]\n",
    "\n",
    "img_arr_notdog = cv2.imread(os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/train/not_hot_dog/197.jpg')))\n",
    "img_arr_notdog = cv2.resize(img_arr_notdog, (img_size,img_size))[:,:,::-1]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_arr_hotdog)\n",
    "plt.title(\"Hot dog\"); plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_arr_notdog)\n",
    "plt.title(\"Not dog\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f69b4f",
   "metadata": {
    "id": "90f69b4f"
   },
   "source": [
    "Instead of loading all the data in advance, we create a data pipeline using an `ImageDataGenerator`. The generator will load in the data files as needed and perform two transformations:\n",
    "- Rescaling pixels to be between [0, 1]\n",
    "- Resizing images to be in `img_size`x`img_size` (160x160)\n",
    "\n",
    "During training for each batch, the images are read from disk on the fly, loaded into memory and then the transformations are applied. Hence, we save on memory, which is one advantage of using the `ImageDataGenerator` instead of the specific augmentation layers (even though there are ways to also load from disk directly, but this goes beyond the scope here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a213ce",
   "metadata": {
    "id": "22a213ce"
   },
   "outputs": [],
   "source": [
    "train_data_dir = os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/train'))\n",
    "test_data_dir = os.path.abspath(os.path.join(path_to_data, os.pardir,'hotdog/test'))\n",
    "batch_size = 128\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Data parameters (DO NOT MODIFY)\n",
    "num_train_samples = 498\n",
    "num_test_samples = 500\n",
    "\n",
    "# Data generators (DO NOT MODIFY)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773633f",
   "metadata": {
    "id": "3773633f"
   },
   "source": [
    "### Defining and running an initial model\n",
    "\n",
    "We define a starting model, which you will need to improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c064ac",
   "metadata": {
    "id": "44c064ac"
   },
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "    Conv2D(32, 3, activation='relu', input_shape=(img_size,img_size,3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2e0ed",
   "metadata": {
    "id": "3fb2e0ed"
   },
   "source": [
    "When training the model, there is a small detail to consider: since we generate data on the fly, the training process doesn't know the total number of data points. Normally, in each epoch, we would take as many steps as needed to get through the whole dataset, given our batch_size. Hence, we have \"training samples\" / \"batch size\" as the number of steps per epoch. Here, we have to manually define that number of steps instead (and we do it in exactly this way, for consistency of the meaning of \"epoch\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdef77",
   "metadata": {
    "id": "08cdef77"
   },
   "outputs": [],
   "source": [
    "model1.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'], \n",
    "              optimizer='rmsprop')\n",
    "model1.fit(train_generator,\n",
    "            steps_per_epoch=num_train_samples // batch_size,\n",
    "            epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb5e9c",
   "metadata": {
    "id": "faeb5e9c"
   },
   "source": [
    "As usual, we evaluate the model. Again, the use of a generator implies only a small change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80281b2",
   "metadata": {
    "id": "a80281b2"
   },
   "outputs": [],
   "source": [
    "test_score = model1.evaluate(test_generator,steps=num_test_samples // batch_size, verbose=0)\n",
    "print('Test loss:', test_score[0])\n",
    "print('Test accuracy:', test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5a7cc",
   "metadata": {
    "id": "6bd5a7cc"
   },
   "source": [
    "### Questions\n",
    "\n",
    "1. Can you improve the model using Transfer Learning? You could use the MobileNet as before, or some completely different pre-trained model, such as one of the different [ResNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet) implementations in TensorFlow.\n",
    "2. Are there other things you can do to improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbac18",
   "metadata": {
    "id": "f2cbac18"
   },
   "source": [
    "### Example answer, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed038f",
   "metadata": {
    "id": "f7ed038f"
   },
   "source": [
    "Using transfer learning as the training dataset is relatively small for the application (here: `MobileNet`)\n",
    "\n",
    "Since hotdog images probably have the same low-level details as that of the `ImageNet` dataset, such transfer learning should be feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bde2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa013e60",
   "metadata": {
    "id": "aa013e60"
   },
   "outputs": [],
   "source": [
    "mobilenet = MobileNet(weights='imagenet', include_top=False, input_shape = (160,160,3))\n",
    "mobilenet.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c812d9",
   "metadata": {},
   "source": [
    "Note that mobilenet expects images to be scaled in the interval $[-1,1]$ [(see the documentation here)](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/preprocess_input). Hence, we modify our `ImageDataGenerator` somewhat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e196d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=2. / 255 - 1)\n",
    "test_datagen = ImageDataGenerator(rescale=2. / 255 - 1)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f26e0",
   "metadata": {
    "id": "341f26e0"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    mobilenet,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484db9c4",
   "metadata": {
    "id": "484db9c4"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'], \n",
    "              optimizer=\"rmsprop\")\n",
    "model.fit(train_generator,\n",
    "            steps_per_epoch=num_train_samples // batch_size,\n",
    "            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa7f8f",
   "metadata": {
    "id": "aefa7f8f"
   },
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test_generator,steps=num_test_samples // batch_size, verbose=0)\n",
    "print('Test loss:', test_score[0])\n",
    "print('Test accuracy:', test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a7bb0",
   "metadata": {
    "id": "822a7bb0"
   },
   "source": [
    "Compared to the original model, which contains 1.2 million parameters to train on 498 images, the transfer learning model contains 3.2 million parameters, but only 65.7 thousand are trainable. As a result, the performance improves significantly (even though there is quite a bit of overfitting happening, which we may want to address in a second step)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24cfe5",
   "metadata": {
    "id": "9a24cfe5"
   },
   "source": [
    "### Example answer, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8006173",
   "metadata": {
    "id": "a8006173"
   },
   "source": [
    "We can try to unfreeze some layers close to the top of pre-trained `MobileNet`, to adapt the representation more closely to our hotdog/notdog images. In particular, we only freeze the layers up to the last, and keep the last one unfrozen: (note - always avoid unfreezing `BatchNormalization` layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a468745",
   "metadata": {
    "id": "7a468745"
   },
   "outputs": [],
   "source": [
    "mobilenet.trainable = True\n",
    "# Freeze layers in the base model except the last one\n",
    "for layer in mobilenet.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87807464",
   "metadata": {
    "id": "87807464"
   },
   "source": [
    "The model can be defined as before. Notice how we now have around 2,000 more parameters that are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392ea39",
   "metadata": {
    "id": "7392ea39"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    mobilenet,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43839d1c",
   "metadata": {
    "id": "43839d1c"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'], \n",
    "              optimizer=\"rmsprop\")\n",
    "model.fit(train_generator,\n",
    "            steps_per_epoch=num_train_samples // batch_size,\n",
    "            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b568092",
   "metadata": {
    "id": "4b568092"
   },
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test_generator,steps=num_test_samples // batch_size, verbose=0)\n",
    "print('Test loss:', test_score[0])\n",
    "print('Test accuracy:', test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e103b5",
   "metadata": {
    "id": "15e103b5"
   },
   "source": [
    "By unfreezing layers close to the top, more parameters have become trainable. This can be helpful in training, but we don't see a vast improvement in this case (if any)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_env",
   "language": "python",
   "name": "adl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
