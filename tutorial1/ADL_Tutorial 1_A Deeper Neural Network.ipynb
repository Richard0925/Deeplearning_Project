{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43d73f0",
   "metadata": {},
   "source": [
    "# Programming a deep feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5f1fc",
   "metadata": {},
   "source": [
    "This notebook is based on a fabulous [Kaggle tutorial by DATAI](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners) and uses the \"sign language digits data set\", also found through the link.\n",
    "\n",
    "We start by loading the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d5101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4243e",
   "metadata": {},
   "source": [
    "## 1. The dataset (this part is identical to the logistic regression exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f83c1",
   "metadata": {},
   "source": [
    "The dataset contains 64x64 images of the signs used to represent the ten digits, 0-9. Indexes 204 to 408 of the dataset show the sign for zero and indexes 822 to 1027 show the sign for one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb919a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"digits_X.npy\")\n",
    "y = np.load(\"digits_y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49065d90",
   "metadata": {},
   "source": [
    "Each value of `X` is a matrix with pixel values, while each value of `y` is a vector representing the value of the digit (one-hot encoded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e32a988f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[204].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a21b785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c006c",
   "metadata": {},
   "source": [
    "We can, of course, display the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4da45e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_size = 64\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X[201].reshape(img_size, img_size))\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X[829].reshape(img_size, img_size))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc147df",
   "metadata": {},
   "source": [
    "We only need the zeros and ones for our purposes. Hence, start by gathering only the relevant X-variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3b43afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X[204:409], X[822:1028]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55819fbd",
   "metadata": {},
   "source": [
    "For the ys, we also only want the relevant ones. Moreover, we want to make sure that instead of a vector, we simply have 0 if the digit is zero and 1 if it is one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c2ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros(409 - 204)\n",
    "o = np.ones(1028 - 822)\n",
    "y = np.concatenate((z, o), axis=0).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298d019",
   "metadata": {},
   "source": [
    "With the `reshape`, we make sure that `y` is a vector with two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60642dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411, 64, 64)\n",
      "(411, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca75ea4",
   "metadata": {},
   "source": [
    "Next, we split the data into training and testing with 15% in the test set (you know the drill):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf7efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349, 64, 64)\n",
      "(349, 1)\n",
      "(62, 64, 64)\n",
      "(62, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=172\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622c30e",
   "metadata": {},
   "source": [
    "Finally, we need to \"flatten\" the Xs. Currently, our input is three-dimensional (each observation is a matrix). However, when we run regressions (or train models more generally), we usually have two-dimensional inputs, as it makes things a lot easier to work with. There are exceptions to this of course, specifically when using convolutional neural networks, but let's not get ahead of ourselves.\n",
    "\n",
    "What we will do is to convert each matrix (each observation's X-value) to a vector, simply by stacking all the columns of the matrix. If $X^{(i)} \\in \\mathcal{R}^{n \\times m}$, then the fitting vector $\\hat{X}^{(i)} \\in \\mathcal{R}^{n m}$. So we reshape accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123cb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349, 4096)\n",
      "(349, 4096)\n"
     ]
    }
   ],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
    "print(X_train_flat.shape)\n",
    "print(X_train_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c725fb6",
   "metadata": {},
   "source": [
    "We have 4096 pixels per observation, neatly stacked in a vector. All observations together (349 for train, 62 for test), gives us a (two-dimensional) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499d6fa",
   "metadata": {},
   "source": [
    "## 2. A neural network with an (arbitrarily large) hidden layer of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f547f6",
   "metadata": {},
   "source": [
    "This time, we are creating a model that uses multiple neurons instead of just one. In particular, we will use one hidden layer with `hidden_layer_size` neurons (and the ReLU activation function), and an output layer with a single neuron performing the final binary classification (what activation function do we use here?)\n",
    "\n",
    "The principle approach is the same as before:\n",
    "\n",
    "0. Choose hyperparameters\n",
    "1. Initialize the model parameters $\\theta$ (random weights!)\n",
    "2. Until we cannot improve the cost function anymore (or we reach a certain numer of iterations):\n",
    "- Given your current model parameters, compute the cost function $J(\\theta)$ (forward propagation)\n",
    "- From the cost function, go backward to compute all the relevant derivatives (back-propagation)\n",
    "- Update the parameters: $\\theta := \\theta - \\alpha \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Keep in mind that each neuron in the hidden layer has weights for all $m$ incoming edges (i.e. one for each `dimension`), as well as one bias term. The single neuron in the output layer has one weight for each of its incoming edges, as well as its own bias term.\n",
    "\n",
    "We will usually use dictionaries to store parameters when we have many. The function below is partially completed for you - can you finish it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aa5972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(seed, dimension, hidden_layer_size):\n",
    "    np.random.seed(seed)\n",
    "    parameters = {\n",
    "        \"weights1\": np.random.rand(dimension, hidden_layer_size)\n",
    "        * 0.01,  # Use np.random.rand(dim1,dim2)*0.01, inputing the correct dimensions\n",
    "        \"bias1\": np.zeros(\n",
    "            (1, hidden_layer_size)\n",
    "        ),  # Use np.zeros(shape), inputing the correct shape\n",
    "        \"weights2\": np.random.randn(hidden_layer_size, 1)\n",
    "        * 0.01,  # Use np.random.randn(dim1,dim2)*0.01, inputing the correct dimensions\n",
    "        \"bias2\": np.zeros((1, 1)),\n",
    "    }  # Use np.zeros(shape), inputing the correct shape\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# def initialize_parameters(seed=392,dimension=4096,hidden_layer_size=3):\n",
    "#     np.random.seed(seed)\n",
    "#     parameters = {'weights1': np.random.rand(dimension,hidden_layer_size)*0.01,  # Use np.random.rand(dim1,dim2)*0.01, inputing the correct dimensions\n",
    "#                 'bias1': np.zeros((1,hidden_layer_size)),   # Use np.zeros(shape), inputing the correct shape\n",
    "#                 'weights2': np.random.randn(hidden_layer_size,1)*0.01,   # Use np.random.randn(dim1,dim2)*0.01, inputing the correct dimensions\n",
    "#                 'bias2': np.zeros((1,1))}    # Use np.zeros(shape), inputing the correct shape\n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6a5c8",
   "metadata": {},
   "source": [
    "A quick try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8077456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00696469 0.00286139 0.00226851]\n",
      " [0.00551315 0.00719469 0.00423106]\n",
      " [0.00980764 0.0068483  0.00480932]\n",
      " ...\n",
      " [0.00749327 0.00198134 0.00659555]\n",
      " [0.00293223 0.00258217 0.00602021]\n",
      " [0.00131192 0.00316522 0.00815401]]\n",
      "[[0. 0. 0.]]\n",
      "[[ 0.00625204]\n",
      " [-0.03114263]\n",
      " [-0.01691934]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(\n",
    "    seed=123, dimension=4096, hidden_layer_size=3)\n",
    "print(parameters[\"weights1\"])\n",
    "print(parameters[\"bias1\"])\n",
    "print(parameters[\"weights2\"])\n",
    "print(parameters[\"bias2\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89236afc",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "The forward propagation step is quite similar to what we saw in the logistic regression. Of course, our model for $\\hat{y}$ is now a whole lot more complex. But that doesn't matter: the neural network is essentially a computation graph, so we just go layer by layer, and computations are quite easy at each layer. We will make use of two helper functions to compute the ReLU activation (at the hidden layer) and the logistic sigmoid activation (at the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3df91205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    # for proper vectorization, you might want to look up `np.maximum`\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d234fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    # you have seen this before in logistic regression\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e1684",
   "metadata": {},
   "source": [
    "Remember that we want to make fast computations. Hence, our functions need to be able to take a whole matrix of values and compute the activation for each element (they need to be \"vectorized\"). Try it out for both activation functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f0c9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 3]\n",
      " [0 6 0]\n",
      " [3 0 0]]\n",
      "[[0.73105858 0.11920292 0.95257413]\n",
      " [0.5        0.99752738 0.11920292]\n",
      " [0.95257413 0.26894142 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[1, -2, 3], [0, 6, -2], [3, -1, 0]])\n",
    "print(relu(Z))\n",
    "print(sigma(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8c612",
   "metadata": {},
   "source": [
    "Next comes the actual forward propagation step. Remember that we need to compute:\n",
    "1. For each neuron at the first layer:\n",
    "- $Z^{[1]}$ = the weighted sum of the inputs X, to which we add the bias\n",
    "- $A^{[1]}$ = the actual activation: the neuron's activation function applied to $Z^{[1]}$\n",
    "2. For the neuron at the second layer:\n",
    "- $Z^{[2]}$ = the weighted sum of the inputs $A^{[1]}$, to which we add the bias\n",
    "- $A^{[2]}$ = the actual activation: the neuron's activation function applied to $Z^{[2]}$\n",
    "3. The cost function, given $\\hat{y} = A^{[2]}$: We will stick with what we saw before in binary classification, so $J=\\frac{1}{n}\\sum_{i=1}^n L^{(i)}$ with $L^{(i)} = -y^{(i)} \\log \\hat y^{(i)} - (1-y^{(i)}) (1-\\log \\hat y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3efc7",
   "metadata": {},
   "source": [
    "Aside from the cost, the forward propagation should return the computed Z's and A's in what we call a \"cache\". This is important for back-propagation later down the line.\n",
    "\n",
    "We start with a naive implementation, where we compute the activations for each neuron separately. Do you see what happens in the function below? Can you complete the missing pieces?\n",
    "\n",
    "A few hints:\n",
    "- you might benefit from adding print statements and trying out the function using `forward_propagation_naive(X_train_flat,y_train,parameters,2)`\n",
    "- The input matrix $X$ has dimensions $(n,m)$, where $n$ is the number of observations and $m$ the number of features\n",
    "- The weight matrix $W^{[1]}$ has dimensions $(m,\\text{hidden_layer_size})$\n",
    "- The bias vector $b^{[1]}$ has dimensions $(1,\\text{hidden_layer_size})$\n",
    "- The weight matrix $W^{[2]}$ has dimensions $(\\text{hidden_layer_size},1)$\n",
    "- The bias vector $b^{[2]}$ has dimensions $(1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a273ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_naive(X, y, parameters, hidden_layer_size):\n",
    "    # Layer 1 (hidden layer)\n",
    "\n",
    "    Z1 = np.zeros((X.shape[0], hidden_layer_size))\n",
    "    A1 = np.zeros((X.shape[0], hidden_layer_size))\n",
    "\n",
    "    for neuron in range(hidden_layer_size):\n",
    "        w = parameters[\"weights1\"][\n",
    "            :, neuron\n",
    "        ]  # find the right weight (recall that we stacked our weights with shape (in,out))\n",
    "        b = parameters[\"bias1\"][\n",
    "            0, neuron\n",
    "        ]  # find the right bias term (recall that we stacked our biases with shape (1,out))\n",
    "        z = (\n",
    "            np.dot(X, w) + b\n",
    "        )  # compute z, using np.dot. Think of the correct dimensions!\n",
    "        Z1[:, neuron] = z\n",
    "        A1[:, neuron] = relu(z)\n",
    "\n",
    "    # Layer 2 (output layer)\n",
    "    w = parameters[\"weights2\"]\n",
    "    b = parameters[\"bias2\"]\n",
    "    Z2 = (\n",
    "        np.dot(A1, w) + b\n",
    "    )  # at the second layer, there is only one node. Use np.dot again, and watch out for the correct dimensions\n",
    "    A2 = sigma(Z2)\n",
    "\n",
    "    # Compute the cost\n",
    "    yHat = A2  # Complete\n",
    "    cost = (\n",
    "        np.sum(-y * np.log(yHat) - (1 - y) * np.log(1 - yHat)) / X.shape[0]\n",
    "    )  # Complete\n",
    "\n",
    "    # Compute the cache\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "\n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24b462",
   "metadata": {},
   "source": [
    "Try it out. If there are no mistake, the code below should print out 0.7387257645994343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f49fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7387257645994343\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(seed=123, dimension=4096, hidden_layer_size=3)\n",
    "cost, _ = forward_propagation_naive(X_train_flat, y_train, parameters, 3)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48a7e9",
   "metadata": {},
   "source": [
    "We know that vectorization is faster, so we will vectorize not just on the observations, but also on the neurons within a layer (and then see that this is quite a bit faster). Can you complete the function?\n",
    "\n",
    "A few additional hints here:\n",
    "- Each neuron has its own total input z for each observations. Hence, $Z^{[1]}$ should have dimensions $(n,\\text{hidden_layer_size})$\n",
    "- The same logic holds for the neuron at the second layer. Hence, $Z^{[2]}$ should have dimensions $(n,1)$\n",
    "- The activation matrix $A^{[l]}$ has the same dimensions as the total input matrix $Z^{[l]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db6bc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, y, parameters):\n",
    "    # Layer 1 (hidden layer)\n",
    "    Z1 = np.dot(X, parameters[\"weights1\"]) + parameters[\"bias1\"]  # Use np.dot!\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # Layer 2 (output layer)\n",
    "    Z2 = np.dot(A1, parameters[\"weights2\"]) + parameters[\"bias2\"]  # Use np.dot!\n",
    "    A2 = sigma(Z2)\n",
    "\n",
    "    # Compute the cost - this is exactly as before!\n",
    "    yHat = A2\n",
    "    cost = np.sum(-y * np.log(yHat) - (1 - y) * np.log(1 - yHat)) / X.shape[0]\n",
    "\n",
    "    # Compute the cache\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "\n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb57dd",
   "metadata": {},
   "source": [
    "Try it out. If there are no mistake, the code below should print out\n",
    "1. 0.7387257645994343\n",
    "1. (349,3)\n",
    "1. (349,3)\n",
    "1. (349,1)\n",
    "1. (349,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87b180cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7387257645994345\n",
      "(349, 3)\n",
      "(349, 3)\n",
      "(349, 1)\n",
      "(349, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(seed=123, dimension=4096, hidden_layer_size=3)\n",
    "cost, cache = forward_propagation(X_train_flat, y_train, parameters)\n",
    "print(cost)\n",
    "print(cache[\"Z1\"].shape)\n",
    "print(cache[\"A1\"].shape)\n",
    "print(cache[\"Z2\"].shape)\n",
    "print(cache[\"A2\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80773e10",
   "metadata": {},
   "source": [
    "Let's now compare the difference in computation time. What we will do is to create 200 sets of initial parameters for a network of width 10 and apply forward propagation once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1538f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive: Cost = 0.7061321562751905, computation time = 101.50881999999983ms\n",
      "Vectorized: Cost = 0.7061321562751905, computation time = 21.48927499999973ms\n"
     ]
    }
   ],
   "source": [
    "iterations = 200\n",
    "time_naive = 0\n",
    "cost_naive = 0\n",
    "time_vectorized = 0\n",
    "cost_vectorized = 0\n",
    "\n",
    "for it in range(iterations):\n",
    "    parameters = initialize_parameters(\n",
    "        seed=np.random.randint(1), dimension=4096, hidden_layer_size=10\n",
    "    )\n",
    "    # Running things with a for-loop:\n",
    "    tic = time.process_time()\n",
    "    cost, _ = forward_propagation_naive(X_train_flat, y_train, parameters, 10)\n",
    "    toc = time.process_time()\n",
    "    time_naive += 1000 * (toc - tic)\n",
    "    cost_naive += cost\n",
    "    # Running things \"vectorized\":\n",
    "    tic = time.process_time()\n",
    "    cost, _ = forward_propagation(X_train_flat, y_train, parameters)\n",
    "    toc = time.process_time()\n",
    "    time_vectorized += 1000 * (toc - tic)\n",
    "    cost_vectorized += cost\n",
    "\n",
    "print(\n",
    "    \"Naive: Cost = \"\n",
    "    + str(cost_naive / iterations)\n",
    "    + \", computation time = \"\n",
    "    + str(time_naive / iterations)\n",
    "    + \"ms\"\n",
    ")\n",
    "print(\n",
    "    \"Vectorized: Cost = \"\n",
    "    + str(cost_vectorized / iterations)\n",
    "    + \", computation time = \"\n",
    "    + str(time_vectorized / iterations)\n",
    "    + \"ms\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f810c",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "We move onto the second step of our update: finding the gradients. Make sure you use the chain rule. We will discuss in the tutorial how to derive the derivatives, but for the programming part, the relevant computations can be found below:\n",
    "- `dZ2` $= \\nabla_{Z^{[2]}} J = \\frac{1}{n}(A^{[2]} - y)$  (this should give you a $(n,1)$ matrix - why?)\n",
    "- `dW2` $=\\nabla_{W^{[2]}} J  = (A^{[1]})^T  (\\nabla_{Z^{[2]}} J)$ (this should give you a $(\\text{hidden_layer_size},1)$ matrix - why?)\n",
    "- `db2` $=\\nabla_{b^{[2]}} J = \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_1^{[2](i)}}$ (you are summing up over the entries of dZ2)\n",
    "- `dZ1` $= \\nabla_{Z^{[1]}} J = (\\nabla_{Z^{[2]}} J) (W^{[2]})^T \\circ E^{[1]}$. Here, $\\circ$ is element-wise multiplication and $E^{[1]}$ is a matrix of the same dimensions as $Z^{[1]}$ that is 1 when the entry is positive and 0 otherwise (this should give you a $(n,\\text{hidden_layer_size})$ matrix - why?)\n",
    "- `dW1` $=\\nabla_{W^{[1]}} J  = (X^T)(\\nabla_{Z^{[1]}} J)$ (this should give you a $(m,\\text{hidden_layer_size})$ matrix - why?)\n",
    "- `db1` = $\\left[ \\nabla_{b^{[1]}_1} J, \\nabla_{b^{[1]}_2} J,..., \\nabla_{b^{[1]}_{\\text{hidden_layer_size}}} J \\right] = \\left[\\sum_{i=1}^n \\frac{\\partial J}{\\partial z_1^{[1](i)}}, \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_2^{[1](i)}}, ..., \\sum_{i=1}^n \\frac{\\partial J}{\\partial z_{\\text{hidden_layer_size}}^{[1](i)}} \\right]$ (you are summing up over **one** of the axes of dZ1 - be careful to choose the right one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b9a78",
   "metadata": {},
   "source": [
    "A final hint about computing $E^{[1]}$. See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d83b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.array([[1, -2, 3], [0, 6, -2], [3, -1, 0]])\n",
    "\n",
    "np.where(Z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da48a9",
   "metadata": {},
   "source": [
    "We can now define the back-propagation step, which returns the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ece447e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, y, parameters, cache):\n",
    "    dZ2 = (cache[\"A2\"] - y) / X.shape[0]  # Complete\n",
    "    dW2 = np.dot(cache[\"A1\"].T, dZ2)  # Complete\n",
    "    db2 = np.sum(dZ2, axis=0)  # Complete\n",
    "    dZ1 = np.dot(dZ2, parameters[\"weights2\"].T) * np.where(\n",
    "        cache[\"Z1\"] > 0, 1, 0\n",
    "    )  # Complete (349,3)\n",
    "    dW1 = np.dot(X.T, dZ1)  # Complete\n",
    "    db1 = np.sum(\n",
    "        dZ1, axis=0, keepdims=True\n",
    "    )  # make sure to sum up over the right axis (see the np.sum documentation), and to set keepdims=True\n",
    "    grads = {\"weights1\": dW1, \"bias1\": db1, \"weights2\": dW2, \"bias2\": db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bfa61",
   "metadata": {},
   "source": [
    "If everything is programmed correctly, the below code should print out\n",
    "\n",
    "1. -0.000508086877680383\n",
    "1. (4096, 3)\n",
    "1. (1, 3)\n",
    "1. (3, 1)\n",
    "1. (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d982a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0005080868776803828\n",
      "(4096, 3)\n",
      "(1, 3)\n",
      "(3, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(seed=123, dimension=4096, hidden_layer_size=3)\n",
    "cost, cache = forward_propagation(X_train_flat, y_train, parameters)\n",
    "grads = back_propagation(X_train_flat, y_train, parameters, cache)\n",
    "print(grads[\"weights1\"][0, 0])\n",
    "print(grads[\"weights1\"].shape)\n",
    "print(grads[\"bias1\"].shape)\n",
    "print(grads[\"weights2\"].shape)\n",
    "print(grads[\"bias2\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817938b",
   "metadata": {},
   "source": [
    "### Putting it together: parameter updating and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df55ce",
   "metadata": {},
   "source": [
    "In each iteration of our learning procedure, we update the parameters, hopefully moving closer towards the optimum. How we update the parameters is determined by the gradient, as well as the learning rate (a hyper-parameter). Let's define one update step:\n",
    "1. Compute the `forward_propagation` step (returning `cost` and `cache`)\n",
    "1. Compute the `back_propagation` step (using `cache` from `forward_propagation`)\n",
    "1. Update each entry in `parameters` as follows: $\\theta := \\theta - \\alpha \\nabla_{\\theta} J$ (Because we made sure above that the shapes are \"in the right way\", we don't have to worry about individual parameters, but can update whole groups - also, we made sure that the parameters and their gradients are referenced in the same way in both dictionaries. Note that $\\alpha$ is the learning rate)\n",
    "1. Return the updated dictionary `parameters` and the `cost` from `forward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0a82d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update(X, y, parameters, learning_rate):\n",
    "    cost, cache = forward_propagation(X, y, parameters)  # Complete\n",
    "    grads = back_propagation(X, y, cache, parameters)  # Complete\n",
    "    for entry in parameters:\n",
    "        parameters[entry] = parameters[entry] - grads[entry] * learning_rate  # Complete\n",
    "        return parameters, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5141b4",
   "metadata": {},
   "source": [
    "We can now train our model, by running the parameter update multiple times. We will use 3 neurons at the hidden layer, a learning rate of 0.01 and run the algorithm for 2,500 iterations. Can you adjust the function below? Make sure to initialize the parameters with our custom-made function. Also, each time you run the parameter-update, store the resulting `cost` in a list `cost_list`. At the end, return the final `parameter` dictionary and the `cost_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbc2cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X, y, hidden_layer_size=3, learning_rate=0.01, iterations=2500, verbose=True):\n",
    "    parameters = initialize_parameters(seed = np.random.randint(1,300), dimension=X.shape[1], hidden_layer_size=hidden_layer_size) # Initialize the parameters\n",
    "    cost_list = []\n",
    "    for it in range(iterations):\n",
    "        parameters, cost = parameter_update(X, y, parameters, learning_rate) # for each iteration, update the parameters using forward and back propagation\n",
    "        cost_list.append(cost) # Also, make sure to add the cost to the cost_list\n",
    "        if verbose:\n",
    "            print('Cost after iteration %i: %f' % (it, cost))\n",
    "    return parameters, cost_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37873d5d",
   "metadata": {},
   "source": [
    "Now, train the model and display the training loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815d801",
   "metadata": {},
   "source": [
    "def model_training(X,y,hidden_layer_size=3,learning_rate=0.01,iterations=2500,verbose=True):\n",
    "    parameters = initialize_parameters(seed=np.random.randint(1),dimension=X.shape[1],hidden_layer_size=hidden_layer_size) # Initialize the parameters\n",
    "    cost_list = []\n",
    "    for it in range(iterations):\n",
    "        parameters,cost = parameter_update(X,y,parameters,learning_rate) # for each iteration, update the parameters using forward and back propagation\n",
    "        cost_list.append(cost)  # Also, make sure to add the cost to the cost_list\n",
    "        if verbose:\n",
    "            print('Cost after iteration %i: %f' %(it,cost))\n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "39fdda19",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'A2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parameters, cost_list \u001b[38;5;241m=\u001b[39m model_training(X_train_flat, y_train)\n",
      "Cell \u001b[0;32mIn [59], line 5\u001b[0m, in \u001b[0;36mmodel_training\u001b[0;34m(X, y, hidden_layer_size, learning_rate, iterations, verbose)\u001b[0m\n\u001b[1;32m      3\u001b[0m cost_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m----> 5\u001b[0m     parameters, cost \u001b[38;5;241m=\u001b[39m \u001b[43mparameter_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# for each iteration, update the parameters using forward and back propagation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     cost_list\u001b[38;5;241m.\u001b[39mappend(cost) \u001b[38;5;66;03m# Also, make sure to add the cost to the cost_list\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn [60], line 3\u001b[0m, in \u001b[0;36mparameter_update\u001b[0;34m(X, y, parameters, learning_rate)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameter_update\u001b[39m(X, y, parameters, learning_rate):\n\u001b[1;32m      2\u001b[0m     cost, cache \u001b[38;5;241m=\u001b[39m forward_propagation(X, y, parameters)  \u001b[38;5;66;03m# Complete\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Complete\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m      5\u001b[0m         parameters[entry] \u001b[38;5;241m=\u001b[39m parameters[entry] \u001b[38;5;241m-\u001b[39m grads[entry] \u001b[38;5;241m*\u001b[39m learning_rate  \u001b[38;5;66;03m# Complete\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [57], line 2\u001b[0m, in \u001b[0;36mback_propagation\u001b[0;34m(X, y, parameters, cache)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mback_propagation\u001b[39m(X,y,parameters,cache):\n\u001b[0;32m----> 2\u001b[0m     dZ2 \u001b[38;5;241m=\u001b[39m (\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m# Complete\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     dW2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mT, dZ2)\u001b[38;5;66;03m# Complete\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     db2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Complete\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'A2'"
     ]
    }
   ],
   "source": [
    "parameters, cost_list = model_training(X_train_flat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_list)), cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b362b9",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "We don't just want to train a neural network, we also want to use it to make predictions. For this purpose, we create a `predict` function, that takes an input X, as well as the parameters of the trained model.\n",
    "\n",
    "Don't worry about computing the prediction - we have already done so, when we implemented the forward propagation. Note that forward propagation takes as input both an `X` and a `y`, but we don't care about the cost (only about the `yHat = cache['A2']`, so we can give an empty `y`, as long as it has the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b97116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    _, cache = forward_propagation(X, np.zeros((X.shape[0], 1)), parameters)\n",
    "    yHat =    # Get yHat from the cache\n",
    "    y_prediction =      # Make a prediction - when yHat > 0.5, assume 1, otherwise 0\n",
    "    return y_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bc17c",
   "metadata": {},
   "source": [
    "Let's see how well our predictions perform, both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd72c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_test = predict(X_test_flat, parameters)\n",
    "y_prediction_train = predict(X_train_flat, parameters)\n",
    "\n",
    "print(\n",
    "    \"train accuracy: {} %\".format(\n",
    "        100 - np.mean(np.abs(y_prediction_train - y_train)) * 100\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"test accuracy: {} %\".format(\n",
    "        100 - np.mean(np.abs(y_prediction_test - y_test)) * 100\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
