{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfec678",
   "metadata": {},
   "source": [
    "# Programming a logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2729468",
   "metadata": {},
   "source": [
    "This notebook is based on a fabulous [Kaggle tutorial by DATAI](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners) and uses the \"sign language digits data set\", also found through the link.\n",
    "\n",
    "We start by loading the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0cb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4243e",
   "metadata": {},
   "source": [
    "## 1. The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f83c1",
   "metadata": {},
   "source": [
    "The dataset contains 64x64 images of the signs used to represent the ten digits, 0-9. Indexes 204 to 408 of the dataset show the sign for zero and indexes 822 to 1027 show the sign for one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb919a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('digits_X.npy')\n",
    "y = np.load('digits_y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49065d90",
   "metadata": {},
   "source": [
    "Each value of `X` is a matrix with pixel values, while each value of `y` is a vector representing the value of the digit (one-hot encoded):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[204].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[204]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c006c",
   "metadata": {},
   "source": [
    "We can, of course, display the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da45e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X[204].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X[822].reshape(img_size, img_size))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc147df",
   "metadata": {},
   "source": [
    "We only need the zeros and ones for our purposes. Hence, start by gathering only the relevant X-variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X[204:409], X[822:1028] ), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55819fbd",
   "metadata": {},
   "source": [
    "For the ys, we also only want the relevant ones. Moreover, we want to make sure that instead of a vector, we simply have 0 if the digit is zero and 1 if it is one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros(409-204)\n",
    "o = np.ones(1028-822)\n",
    "y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298d019",
   "metadata": {},
   "source": [
    "With the `reshape`, we make sure that `y` is a vector with two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60642dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca75ea4",
   "metadata": {},
   "source": [
    "Next, we split the data into training and testing with 15% in the test set (you know the drill):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=172)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622c30e",
   "metadata": {},
   "source": [
    "Finally, we need to \"flatten\" the Xs. Currently, our input is three-dimensional (each observation is a matrix). However, when we run regressions (or train models more generally), we usually have two-dimensional inputs, as it makes things a lot easier to work with. There are exceptions to this of course, specifically when using convolutional neural networks, but let's not get ahead of ourselves.\n",
    "\n",
    "What we will do is to convert each matrix (each observation's X-value) to a vector, simply by stacking all the columns of the matrix. If $X^{(i)} \\in \\mathcal{R}^{n \\times m}$, then the fitting vector $\\hat{X}^{(i)} \\in \\mathcal{R}^{n m}$. So we reshape accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flat = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n",
    "print(X_train_flat.shape)\n",
    "print(X_train_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c725fb6",
   "metadata": {},
   "source": [
    "We have 4096 pixels per observation, neatly stacked in a vector. All observations together (349 for train, 62 for test), gives us a (two-dimensional) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4838cbd",
   "metadata": {},
   "source": [
    "## 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7e4e8",
   "metadata": {},
   "source": [
    "As you know, we are going to create a model $\\hat{y}^{(i)} = \\sigma(X^{(i)} w + b) = \\sigma(X_1^{(i)}  w_1  + X_2^{(i)} w_2  + ... + X_{4096}^{(i)} w_{4096}  + b)$. \"Training\" the model, means that we are finding the \"right\" $w_j$'s and $b$. The right ones are those that minimize our training loss (we'll turn back to that in a bit).\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We have to start somewhere. In particular, we are going to initialize our 4097 parameters first. For the 4096 $w_j$'s, we use small random values. The variable $b$ (\"bias\") will be initialized to 0. We will discuss later in more detail the correct strategy for initialization.\n",
    "\n",
    "To do this systematically, we define a function to call.\n",
    "Can you complete this function using `np.random.randn(dimension,1)*0.01`? The function should return `w` (a vector) and `b` (a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(seed=392,dimension=4096):\n",
    "    np.random.seed(seed)\n",
    "    w = 0.01*np.random.randn(dimension,1)\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6a5c8",
   "metadata": {},
   "source": [
    "A quick try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8077456",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = initialize_parameters(seed=123,dimension=4096)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f0bb1",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "When training a neural network, in each step, we start with computing the cost that we aim to minimize, given our current choice of parameters $w$ and $b$. This cost is made up of the \"losses\" on each training observation. In particular, we have the loss on observation $i$ that is\n",
    "$$\n",
    "L^{(i)} = -y^{(i)} \\log \\hat y^{(i)} - (1-y^{(i)}) (1-\\log \\hat y^{(i)})\n",
    "$$\n",
    "and that sums up to our cost\n",
    "$$\n",
    "J = \\frac{1}{n}\\sum_{i=1}^n L^{(i)}.\n",
    "$$\n",
    "Remember also, that we had $\\hat{y}^{(i)} = \\sigma(X^{(i)} w + b)$.\n",
    "\n",
    "This step is called \"forward propagation\" (the reason is easy to see if you look at the computation graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b19c8",
   "metadata": {},
   "source": [
    "Before we actually implement our forward propagation function, however, we have to discuss computation speed. We could, in principal, consider every observation individually in a for-loop, and for each observation, we can consider each pixel in an inner for-loop. However, the following example will show you that this is quite slow compared to the \"vectorized\" implementation we can get in `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8168acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running things with two for-loops:\n",
    "tic = time.process_time()\n",
    "cost = 0\n",
    "for i in range(X_train_flat.shape[0]):\n",
    "    z = b\n",
    "    for j in range(X_train_flat.shape[1]):\n",
    "        z += w[j]*X_train_flat[i,j]\n",
    "    yHat = 1/(1 + np.exp(-z))\n",
    "    cost += -y_train[i]*np.log(yHat) - (1-y_train[i])*np.log(1-yHat)\n",
    "    cost /= X_train_flat.shape[0]\n",
    "toc = time.process_time()\n",
    "print (\"For-loops: Cost = \" + str(cost) + \", computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "# Running things \"vectorized\":\n",
    "tic = time.process_time()\n",
    "z = np.dot(X_train_flat,w) + b # Can you see why we do np.dot this way round?\n",
    "yHat = 1/(1 + np.exp(-z))\n",
    "cost = np.sum(-y_train*np.log(yHat) - (1-y_train)*np.log(1-yHat)) / X_train_flat.shape[0]\n",
    "toc = time.process_time()\n",
    "print (\"Vectorized: Cost = \" + str(cost) + \", computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574de3d",
   "metadata": {},
   "source": [
    "From this, it should be clear that we want to always vectorize our computations as much as possible! Let's now use the above to implement a forward propagation function.\n",
    "\n",
    "Can you complete the function and return both the `cost`, as well as the predicted `yHat`? (Hint: all the relevant parts are already in the computation speed comparison and only require minimal changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ee6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(w,b,X,y):\n",
    "    z = np.dot(X,w) + b # Can you see why we do np.dot this way round?\n",
    "    yHat = 1/(1 + np.exp(-z))\n",
    "    cost = np.sum(-y*np.log(yHat) - (1-y)*np.log(1-yHat))/X.shape[0]\n",
    "    return cost, yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173393a9",
   "metadata": {},
   "source": [
    "Note that we are not just storing the loss, but also the $\\hat y$ we computed. We will need those later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed366c",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "Now that we have found the cost, we want to minize that. In order to do so, we use gradient descent. But for that, we first need the derivatives of the cost to our $w$'s and to $b$. Remember that computing derivatives from a computation graph, we start at the end and move backward. Hence the name \"back-propagation\". Going through all the steps, we arrive at the following derivatives:\n",
    "$$ \\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n (  \\hat y^{(i)} - y^{(i)} ) X^{(i)}_j,$$\n",
    "$$\\nabla_{w} J = \\frac{1}{n} X^T (\\hat y - y ),$$\n",
    "$$\\nabla_{b} J = \\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (  \\hat y^{(i)} - y^{(i)} ).$$\n",
    "\n",
    "**Exercise: Verify (using the computation graph) that the derivatives are correct**.\n",
    "\n",
    "Can you complete the function below, using the formulas here? Make sure to return a vector `dw` with all the $\\frac{\\partial J}{\\partial w_j}$'s (in the same shape as the vector `w`!) and a scalar `db` with $\\frac{\\partial J}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(w,b,X,y,yHat):\n",
    "    dw = np.dot(X.T,yHat-y)/X.shape[0]\n",
    "    db = np.sum(yHat - y)/X.shape[0]\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280dfcf2",
   "metadata": {},
   "source": [
    "### Parameter update\n",
    "\n",
    "In each iteration of our learning procedure, we update the parameters, hopefully moving closer towards the optimum. How we update the parameters is determined by the gradient, as well as the learning rate (a hyper-parameter). Let's define one update step:\n",
    "1. Compute the `forward_propagation` step (returning `cost` and `yHat`)\n",
    "1. Compute the `back_propagation` step (using `yHat` from `forward_propagation`)\n",
    "1. Update `w` as follows: $w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}$ (remember that, thanks to vectorization, we can avoid for-loops. Also note that $\\alpha$ is the learning rate)\n",
    "1. Update `b` as follows: $b := b - \\alpha \\frac{\\partial J}{\\partial b}$\n",
    "1. Return `w`, `b`, and `cost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d52756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_update(w,b,X,y,learning_rate):\n",
    "    cost, yHat = forward_propagation(w,b,X,y)\n",
    "    dw, db = back_propagation(w,b,X,y,yHat)\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058788b5",
   "metadata": {},
   "source": [
    "With this in hand, we can now train our model. We will use one final function that takes as input the training data, as well as the hyper-parameters `learning_rate` and `iterations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6912aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X,y,learning_rate=0.005,iterations=300):\n",
    "    w, b = initialize_parameters(seed=392,dimension=X.shape[1]) # start by initializing the parameters w, b\n",
    "    cost_list = []\n",
    "    for it in range(iterations):\n",
    "        w,b,cost = parameter_update(w,b,X,y,learning_rate) # for each iteration, update the parameters using forward and back propagation\n",
    "        cost_list.append(cost)\n",
    "    return w, b, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbf1c8",
   "metadata": {},
   "source": [
    "Let's get training. We will train the model for 300 iterations on the (flattened) training data, then take a look at the cost (also called \"training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a055a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b,cost_list = model_training(X_train_flat,y_train,learning_rate=0.005,iterations=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_list)),cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc09271",
   "metadata": {},
   "source": [
    "Finally, we can use our model to make predictions on the test set. We define another function to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123051f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    z = np.dot(X,w) + b\n",
    "    yHat = 1/(1 + np.exp(-z))\n",
    "    # if yHat is bigger than 0.5, our prediction is sign one\n",
    "    # if yHat is smaller than 0.5, our prediction is sign zero\n",
    "    y_prediction = (yHat > 0.5)\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28f872",
   "metadata": {},
   "source": [
    "Let's now get predictions. Using these predictions, we can evaluate the accuracy of the model (both on the training and on the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_test = predict(w,b,X_test_flat)\n",
    "y_prediction_train = predict(w,b,X_train_flat)\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374d339",
   "metadata": {},
   "source": [
    "Not too shabby for a very simple classification algorithm, right? Let's take a look at a few of the examples where things where **not** classified correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Misclassified zero sign as 1: \" + str(np.where(y_prediction_test - y_test == 1)[0]))\n",
    "print(\"Misclassified one sign as 0: \" + str(np.where(y_prediction_test - y_test == -1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a806db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_env",
   "language": "python",
   "name": "adl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
