{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e20e0d",
   "metadata": {
    "id": "e1e20e0d"
   },
   "source": [
    "In this notebook, we wrapt up time series prediction with RNNs, before moving to a (brief) introduction to NLP. We will see how we can apply different types of models to NLP tasks, from simple bag-of-word models, over RNN-driven sequence models, to transformers. We will also learn how we can use pre-trained transformer models using the ðŸ¤— Hugging Face library.\n",
    "\n",
    "The notebook builds on a combination of the GitHub repositories by [AurÃ©lien GÃ©ron](https://github.com/ageron/handson-ml2) and [Francois Chollet](https://github.com/fchollet/deep-learning-with-python-notebooks), and official TensorFlow and ðŸ¤— Hugging Face tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f6c95",
   "metadata": {
    "id": "116f6c95"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random as rnd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, GRU, Bidirectional, Flatten, TextVectorization, GlobalMaxPooling1D, BatchNormalization, Conv1D, LSTM\n",
    "from tensorflow.keras.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5851f8c",
   "metadata": {
    "id": "f5851f8c"
   },
   "source": [
    "# 1. Back to our timeseries prediction example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f81d0",
   "metadata": {
    "id": "3c5f81d0"
   },
   "source": [
    "## 1.1 Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdce1b",
   "metadata": {
    "id": "9cfdce1b"
   },
   "source": [
    "We start by fetching the data. This is the same code as last time, just put into one cell. If you have the \"climate_data.csv\" file in the same folder as the notebook and are running it locally, set `google = True`. If you are running on Colab, make sure to leave `google = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a1925",
   "metadata": {
    "id": "396a1925"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "google = True\n",
    "\n",
    "if google:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/gdrive')\n",
    "    path = \"gdrive/MyDrive/\"\n",
    "else:\n",
    "    path = \"\"\n",
    "    \n",
    "fname = os.path.join(path+\"climate_data.csv\")\n",
    "with open(fname) as f:\n",
    "    data = f.read()\n",
    "\n",
    "lines = data.split(\"\\n\")\n",
    "header = lines[0].split(\",\")\n",
    "lines = lines[1:]\n",
    "\n",
    "# Create temperature data and predictors\n",
    "temperature = np.zeros((len(lines),))\n",
    "raw_data = np.zeros((len(lines), len(header) - 1))\n",
    "\n",
    "# Convert data into arrays\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(\",\")[1:]] # Date Time is irrelevant, as data is equally spaced, so we drop it\n",
    "    temperature[i] = values[1] # We store the temperature (this is what we will predict)\n",
    "    raw_data[i, :] = values[:] # We store all data points (including the temperature), which will be our predictors\n",
    "    \n",
    "# Define the parts used for training and validation (rest is for testing)\n",
    "train_size = int(0.5 * len(raw_data))\n",
    "valid_size = int(0.25 * len(raw_data))\n",
    "\n",
    "# Standardize the data (using the training mean and standard deviation)\n",
    "mean = raw_data[:train_size].mean(axis=0)\n",
    "std = raw_data[:train_size].std(axis=0)\n",
    "raw_data = (raw_data - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa370c9",
   "metadata": {
    "id": "6aa370c9"
   },
   "source": [
    "# 1.2 Forecasting multiple timesteps: a sequence-to-vector approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652b6d5",
   "metadata": {
    "id": "9652b6d5"
   },
   "source": [
    "So far, we have only forecast one timestep. We will now forecast the temperature for the entire next day, instead. For this, we have to recreate our datasets accordingly. To see how to create the correct dataset, let's start with a simple dummy example. The idea is that we create two datasets, one for input sequences, and one for output sequences, then add them together to have a single dataset to call when fitting the model.\n",
    "\n",
    "**Discuss**: Play around with the input parameters to understand the data generation process:\n",
    "- Why do we need different sequence lengths\n",
    "- How do you control the offset between inputs and targets?\n",
    "- Why do we use a `batch_size` of 1 when generating the individual datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70b077",
   "metadata": {
    "id": "ad70b077"
   },
   "outputs": [],
   "source": [
    "int_sequence = np.arange(18)\n",
    "sequence_length_input = 3\n",
    "sequence_length_output = 2\n",
    "delay = sequence_length_input + 2\n",
    "batch_size = 4\n",
    "\n",
    "def create_multiple_dummy_datasets(start_index, end_index):\n",
    "    dummy_input_data = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=int_sequence[:-delay],\n",
    "        targets=None,\n",
    "        sequence_length=sequence_length_input,\n",
    "        start_index = start_index,\n",
    "        end_index = end_index,\n",
    "        batch_size=1).unbatch() # We create \"unbatched\" datasets, since we want to merge the data one by one\n",
    "    dummy_target_data = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=int_sequence[delay:],\n",
    "        targets=None,\n",
    "        sequence_length=sequence_length_output, # The length of the output sequences may be different from the length of the input sequences\n",
    "        start_index = start_index,\n",
    "        end_index = end_index,\n",
    "        batch_size=1).unbatch() # We create \"unbatched\" datasets, since we want to merge the data one by one\n",
    "    return tf.data.Dataset.zip((dummy_input_data, dummy_target_data)).batch(batch_size) # We merge the two datasets and then batch them\n",
    "\n",
    "train_dummies = create_multiple_dummy_datasets(0, 8)\n",
    "val_dummies = create_multiple_dummy_datasets(8, None)\n",
    "\n",
    "for data in [train_dummies, val_dummies]:\n",
    "    print(\"Next data set:\")\n",
    "    for inputs, targets in data:\n",
    "        print(\"    Next batch:\")\n",
    "        for i in range(targets.shape[0]):\n",
    "            print(\"       input:\",[int(x) for x in inputs[i]], \"| output:\",[int(x) for x in targets[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154ecb1",
   "metadata": {
    "id": "4154ecb1"
   },
   "source": [
    "Once we have understood how to combine two data sets in order to generate input and output sequence, we can scale it up to create our actual datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f033d61",
   "metadata": {
    "id": "8f033d61"
   },
   "outputs": [],
   "source": [
    "sampling_rate = 6\n",
    "sequence_length_input = 120\n",
    "sequence_length_output = 24\n",
    "delay = sampling_rate * (sequence_length_input + 24 - 1)\n",
    "batch_size = 256\n",
    "\n",
    "def create_multiple_datasets(start_index, end_index):\n",
    "    input_data = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data = raw_data[:-delay],\n",
    "        targets= None,\n",
    "        sampling_rate=sampling_rate,\n",
    "        sequence_length=sequence_length_input,\n",
    "        batch_size=1,\n",
    "        start_index=start_index,\n",
    "        end_index=end_index).unbatch()\n",
    "    target_data = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data = temperature[delay:],\n",
    "        targets = None,\n",
    "        sampling_rate=sampling_rate,\n",
    "        sequence_length=sequence_length_output,\n",
    "        batch_size=1,\n",
    "        start_index=start_index,\n",
    "        end_index=end_index).unbatch()\n",
    "    return tf.data.Dataset.zip((input_data, target_data)).shuffle(100).batch(batch_size) # Compared to the dummy data, we also shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee564ca",
   "metadata": {
    "id": "fee564ca"
   },
   "outputs": [],
   "source": [
    "train_data = create_multiple_datasets(0, train_size)\n",
    "val_data = create_multiple_datasets(train_size, train_size + valid_size)\n",
    "test_data = create_multiple_datasets(train_size + valid_size, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5ee47",
   "metadata": {
    "id": "c2d5ee47"
   },
   "source": [
    "Let's take a look at dimensions. Note that our inputs are 120-entry sequences of vectors with dimension 14 each. Our outputs, on the other hand, are 24-dimensional vectors, but not actual sequences. We could create target sequences of vectors (for example, for a sequence-to-sequence approach), but we won't worry about this for now, as it's quite a bit more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43373f06",
   "metadata": {
    "id": "43373f06"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in train_data:\n",
    "    print(\"Input:\",inputs.shape)\n",
    "    print(\"Output:\",targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa306830",
   "metadata": {
    "id": "fa306830"
   },
   "source": [
    "Let's create a model. We are going to predict ten timesteps ahead with a sequence-to-vector model (i.e., we take the sequence `0,...,119` and predict `143,...,166` as a single output, or vector). The key here is that we need to have 24 outputs in the dense layer, corresponding to the length of our target vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fe35b",
   "metadata": {
    "id": "4a4fe35b"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(sequence_length_input, raw_data.shape[-1])),\n",
    "    GRU(20, return_sequences=True),\n",
    "    GRU(20),\n",
    "    Dense(24)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1cc9c",
   "metadata": {
    "id": "81e1cc9c"
   },
   "source": [
    "Note that it will take a bit to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34cd540",
   "metadata": {
    "id": "c34cd540"
   },
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"temp_stv\",save_best_only=True)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "history = model.fit(train_data,\n",
    "                    epochs=8,\n",
    "                    validation_data=val_data,\n",
    "                    callbacks=[checkpoint_cb])\n",
    "\n",
    "model = tf.keras.models.load_model(\"temp_stv\")\n",
    "print(f\"Test MAE: {model.evaluate(test_data)[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a07af",
   "metadata": {
    "id": "cb6a07af"
   },
   "source": [
    "Is the model doing well? We can't compare with the previous one, because we are now estimating temperatures for the whole day rather than just one time point. But you could build very similar baselines as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8269f",
   "metadata": {
    "id": "7aa8269f"
   },
   "source": [
    "# 2. A simple text-classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cQis3SuLIzAg",
   "metadata": {
    "id": "cQis3SuLIzAg"
   },
   "source": [
    "We will next go through a relatively simple NLP task: sentiment analysis. In particular, we take a text, and we predict whether the sentiment of the writer is positive or negative. To train (and test) our model, we will load the IMDB review dataset, which has been pre-labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba35588",
   "metadata": {
    "id": "eba35588"
   },
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JdB3rNz3JPCB",
   "metadata": {
    "id": "JdB3rNz3JPCB"
   },
   "source": [
    "The data comes with a training set and a test set. We will also split off a validation set, and keep the correct folder structure (\"neg\" and \"pos\" subfolders within each of the folders \"train\", \"test\", \"val\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968aa4b3",
   "metadata": {
    "id": "968aa4b3"
   },
   "outputs": [],
   "source": [
    "base_dir = Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    rnd.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XUhZvLmiJhjw",
   "metadata": {
    "id": "XUhZvLmiJhjw"
   },
   "source": [
    "We will use the `tf.data` API that we learned about last time in order to load the revies from our directory when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9aa89f",
   "metadata": {
    "id": "ce9aa89f"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ykQUi34DJtWc",
   "metadata": {
    "id": "ykQUi34DJtWc"
   },
   "source": [
    "Let's take a look at the first batch of data (and the first text + label) within that batch:\n",
    "\n",
    "**Discussion**: What do you notice? What are the data types and do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56de5d8",
   "metadata": {
    "id": "d56de5d8"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b5e12",
   "metadata": {
    "id": "547b5e12"
   },
   "source": [
    "## 2.1 A bag-of-words approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "boLuFoAYJ9jT",
   "metadata": {
    "id": "boLuFoAYJ9jT"
   },
   "source": [
    "We start with a simple \"bag-of-words\" approach, that is, a model that doesn't consider the sequence of the words in the reviews, just their existence. However, to get at least a little bit of contextual information, we will look for \"bigrams\" rather than only individual words. That is, we consider all sequences of two words (\"That is\", \"is we\", \"we consider\", \"consider all\", \"all sequences\", \"sequences of\", \"of two\", \"two words\").\n",
    "\n",
    "**Discussion**: Can you come up with a simple example where a model will likely missclassify the sentiment when only looking at individual words, but correctly classify the sentiment when looking at 2-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b0b54",
   "metadata": {
    "id": "452b0b54"
   },
   "outputs": [],
   "source": [
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"multi_hot\"\n",
    ")\n",
    "text_vectorization.adapt(train_ds.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3QTiIVCPK7wu",
   "metadata": {
    "id": "3QTiIVCPK7wu"
   },
   "source": [
    "From our previous datasets, we derive new datasets that \"vectorize\" the data appropriately, in order to get integers corresponding to the desired bigrams. For this, we use the `.map` functionality together with a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf746a06",
   "metadata": {
    "id": "cf746a06"
   },
   "outputs": [],
   "source": [
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M1-Wa_UBLNLp",
   "metadata": {
    "id": "M1-Wa_UBLNLp"
   },
   "source": [
    "Let's take another look at the transformed data:\n",
    "\n",
    "**Discussion**: Why is the input shape (32,20000)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770449c",
   "metadata": {
    "id": "4770449c"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in binary_2gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9EZ76dv6LVpT",
   "metadata": {
    "id": "9EZ76dv6LVpT"
   },
   "source": [
    "Let's create a simple FFN with an intermediate `Dense` layer, a `Dropout` layer, and the final classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f3b54",
   "metadata": {
    "id": "893f3b54"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(max_tokens,))\n",
    "x = Dense(16, activation=\"relu\")(inputs)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lHy7JMhALg4a",
   "metadata": {
    "id": "lHy7JMhALg4a"
   },
   "source": [
    "Let's now run this model on our data. This will be relatively fast, since we have only a small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bb2c3",
   "metadata": {
    "id": "b02bb2c3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"binary_2gram\",save_best_only=True)]\n",
    "model.fit(binary_2gram_train_ds.cache(), # .cach() saves the whole dataset in memory, which avoids redoing the pre-processing, but is only possible if the dataset isn't too large\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(\"binary_2gram\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38315c5",
   "metadata": {
    "id": "b38315c5"
   },
   "source": [
    "## 2.2 A first sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LyUKezm0LloW",
   "metadata": {
    "id": "LyUKezm0LloW"
   },
   "source": [
    "Next, we want to try a sequential model, that is, a model that captures the (global) order information of the reviews. The most natural approach is an RNN, which is designed exactly for this purpose.\n",
    "\n",
    "Before we build the model, we have to encode our strings as integers. This time, we also specify a `max_length` to ensure that our input sequences are not too long (otherwise, training will be very slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03230a2",
   "metadata": {
    "id": "f03230a2"
   },
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "max_tokens = 20000\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(train_ds.map(lambda x, y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z9h8ws61MHTu",
   "metadata": {
    "id": "z9h8ws61MHTu"
   },
   "source": [
    "Again, we use the `.map` function of our dataset to go through the vectorization process. We also `.prefetch` here, so that the next batch can be pre-processed on the CPU while the GPU trains on the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e4b03",
   "metadata": {
    "id": "483e4b03"
   },
   "outputs": [],
   "source": [
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),  num_parallel_calls=tf.data.AUTOTUNE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Xt1STBBMjn7",
   "metadata": {
    "id": "5Xt1STBBMjn7"
   },
   "source": [
    "Let's take a look at the pre-processed data.\n",
    "\n",
    "**Discussion**: Do you notice a difference to the previous encoding we used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8ba86",
   "metadata": {
    "id": "cce8ba86"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in int_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drrr-C6hMuND",
   "metadata": {
    "id": "drrr-C6hMuND"
   },
   "source": [
    "We next generate our model. We will use a `Bidirectional` `GRU` before our classification layer. We also use the `tf.one_hot` functionality to turn our input vectors into sequences of (one-hot-encoded) word-representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0832e28",
   "metadata": {
    "id": "c0832e28"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = Bidirectional(GRU(32))(embedded)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1NZ-0aIIM7i5",
   "metadata": {
    "id": "1NZ-0aIIM7i5"
   },
   "source": [
    "Next is the training step. Even on Colab, this will take a while, so don't worry too much about actually running this. In fact, we will only get to around 87% test accuracy, worse than the much faster bigram model. So, really, don't worry :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926c5f2",
   "metadata": {
    "id": "9926c5f2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"one_hot_bidir_gru\",save_best_only=True)]\n",
    "model.fit(int_train_ds,\n",
    "          validation_data=int_val_ds,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(\"one_hot_bidir_gru\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3374b",
   "metadata": {
    "id": "f9e3374b"
   },
   "source": [
    "## 2.3 A sequential model with a newly trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XrB2N7JVNcTb",
   "metadata": {
    "id": "XrB2N7JVNcTb"
   },
   "source": [
    "Especially when using sequential models, we generally rather use an embedding of our data than the original data. The denser input-matrix leads to faster training but also, normally, much better performance. Let's try it out, by adding an `Embedding` layer. The `mask_zero=True` ensures that our model skips zero-words (those that exist only because one sequence was shorter than another and we added \"padding\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223d0b3",
   "metadata": {
    "id": "4223d0b3"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = Bidirectional(GRU(32))(embedded)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bn1x1E0NOEj9",
   "metadata": {
    "id": "bn1x1E0NOEj9"
   },
   "source": [
    "While faster than the previous model (thanks, `Embedding`!), this still takes quite some time. Hence, here a sneak preview: the test accuracy is around 0.877, so we are still not beating our bag-of-words model - and it's not really worth spending all this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e007b",
   "metadata": {
    "id": "ed0e007b"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru\",save_best_only=True)]\n",
    "model.fit(int_train_ds,\n",
    "          validation_data=int_val_ds,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(\"embeddings_bidir_gru\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26baa9",
   "metadata": {
    "id": "3d26baa9"
   },
   "source": [
    "## 2.4 A sequential model with a pre-trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dg3wuzNFPOi6",
   "metadata": {
    "id": "Dg3wuzNFPOi6"
   },
   "source": [
    "We will train another model, but this time, we will use a pre-trained embedding, to make our training process faster. In particular, we will use GloVe, one of the two embeddings most commonly used (the other being Text2Vec).\n",
    "\n",
    "Note 1: Unzipping the GloVe files is a bit time-consuming. Don't worry, that's normal.\n",
    "\n",
    "Note 2: If you are running this locally, you may first have to install the `wget` command to make the download work. Alternatively, you can download the zip file directly and load it into your current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f3f15",
   "metadata": {
    "id": "a15f3f15"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mu6sjvdYPmut",
   "metadata": {
    "id": "Mu6sjvdYPmut"
   },
   "source": [
    "Now that we have downloaded and unzipped the GloVe (actually, there are different files, based on the size of the embedding dimension you care for - we will use the 100-dimensional one), we will create a mapping between words and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba1166",
   "metadata": {
    "id": "11ba1166"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W1noueIJP7dx",
   "metadata": {
    "id": "W1noueIJP7dx"
   },
   "source": [
    "Take a look, for example, at the word \"the\". The `embeddings_index` will return the value of \"the\" on each of the 100 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81991d4d",
   "metadata": {
    "id": "81991d4d"
   },
   "outputs": [],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svyW3hh-QD43",
   "metadata": {
    "id": "svyW3hh-QD43"
   },
   "source": [
    "Next, we create a matrix that maps each word into its embedding. We use this matrix to initialize a new `Embedding` layer, and we make sure that this layer has `trainable=False` (since we don't want to overwrite the embedding!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9e2ca",
   "metadata": {
    "id": "0bb9e2ca"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NRQYItJNQeK9",
   "metadata": {
    "id": "NRQYItJNQeK9"
   },
   "source": [
    "Next up, defining the model. There is no difference here, other than the dimensionality of the embedding.\n",
    "\n",
    "**Discussion**: Why are there so many non-trainable parameters here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee022e8",
   "metadata": {
    "id": "dee022e8"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = Bidirectional(GRU(32))(embedded)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NN326EZNRY_j",
   "metadata": {
    "id": "NN326EZNRY_j"
   },
   "source": [
    "Let's train this again. Things are a bit faster this time, because we don't have to train the `Embedding` layer from scratch. Still not fast, though, thanks to the recurrence. In case training takes too long, here the test accuracy: 0.876\n",
    "\n",
    "**Discussion**:\n",
    "- Can you comment on the performance of the model and what do you think the drivers are?\n",
    "- When are pre-trained embeddings most valuable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dd8eb",
   "metadata": {
    "id": "6a8dd8eb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"glove_embeddings_bidir_gru\",save_best_only=True)]\n",
    "model.fit(int_train_ds,\n",
    "          validation_data=int_val_ds,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(\"glove_embeddings_bidir_gru\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0672a05",
   "metadata": {
    "id": "b0672a05"
   },
   "source": [
    "## 2.5 A Transformer model for text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xeqEDAFERxus",
   "metadata": {
    "id": "xeqEDAFERxus"
   },
   "source": [
    "We will now use a transformer model (or, at least, the encoder part of a transformer). Don't worry too much about the details here. The two things to really note are that\n",
    "- We are subsetting the `tf.keras.layers.Layer` class, so we are creating a new type of layer\n",
    "- While there is no transformer layer per default in TensorFlow, the `MultiHeadAttention` layer exists and so we can call this up directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3383316",
   "metadata": {
    "id": "a3383316"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjFCKYx-Spp0",
   "metadata": {
    "id": "GjFCKYx-Spp0"
   },
   "source": [
    "Next, we create the `PositionalEmbedding`, that is added to give order information to the model. Again, don't worry too much about the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa007c",
   "metadata": {
    "id": "42aa007c"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nMzPHr-zS052",
   "metadata": {
    "id": "nMzPHr-zS052"
   },
   "source": [
    "We now build our model. We have to specify the size of the vocabulary and the sequences, the dimension of our positional embedding, the number of heads in the multi-head attention, and the number of Dense layers at the start of the transformer encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea899f",
   "metadata": {
    "id": "f1ea899f"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 500\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R8Vtk_e4TZBf",
   "metadata": {
    "id": "R8Vtk_e4TZBf"
   },
   "source": [
    "Note that this network is much bigger than the previous one (5 million trainable parameters versus 20 thousand), and even bigger than the one where we trained our own `Embedding`. You will generally also need a few more epochs until convergence. If you don't have the time, here the test accuracy: 0.884\n",
    "\n",
    "**Discussion**: How does this compare to the previous models we have seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192632b5",
   "metadata": {
    "id": "192632b5"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"transformer_encoder\",save_best_only=True)]\n",
    "model.fit(int_train_ds,\n",
    "          validation_data=int_val_ds,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(\"transformer_encoder\",custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                                                                      \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lFutpWfyZddr",
   "metadata": {
    "id": "lFutpWfyZddr"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"transformer_encoder\",custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                                                                      \"PositionalEmbedding\": PositionalEmbedding})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072f9ee",
   "metadata": {
    "id": "7072f9ee"
   },
   "source": [
    "# 3. Pre-trained transformer models using ðŸ¤— Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CmPncsDKTs0x",
   "metadata": {
    "id": "CmPncsDKTs0x"
   },
   "source": [
    "We now explore repeating the same task with a pre-trained model from the ðŸ¤— Hugging Face library. You will probably have to start with some installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628e05a",
   "metadata": {
    "id": "8628e05a"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d352c",
   "metadata": {
    "id": "c69d352c"
   },
   "source": [
    "## 3.1 A default model for sentiment classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QzQLwk5IUA1o",
   "metadata": {
    "id": "QzQLwk5IUA1o"
   },
   "source": [
    "ðŸ¤— Hugging Face provides many transformer-based neural network architectures (such as BERT, GPT-2, RoBERTa, XLM, DistilBert, etc.) for NLP, including many pretrained models. It relies on either `TensorFlow` or `PyTorch` (even though the default is `PyTorch`, so if you want to go deeper into NLP it might be worth taking a look at that).\n",
    "\n",
    "For simple applications, you can use the pre-defined `pipeline` module from the `transformers` package. In this case, there is no fine-tuning of the models, but you can use whatever is there off the bat (and there are [a lot of models](https://huggingface.co/models)).\n",
    "\n",
    "We simply have to load the \"right\" pipeline (in our case, `'sentiment-analysis'`) and choose the underlying transformer model. If the model doesn't work with the pipeline, you'll get a warning. If you don't specify a model, it will load a default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f7f5d",
   "metadata": {
    "id": "e39f7f5d"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69e750",
   "metadata": {
    "id": "dd69e750"
   },
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3hZVx1AV4Mc",
   "metadata": {
    "id": "u3hZVx1AV4Mc"
   },
   "source": [
    "Let's try it out with a simple case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_y_IdY-FV7S_",
   "metadata": {
    "id": "_y_IdY-FV7S_"
   },
   "outputs": [],
   "source": [
    "sentiment_pipeline(\"The movie was terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qrMwdh6VV9jX",
   "metadata": {
    "id": "qrMwdh6VV9jX"
   },
   "source": [
    "Next, we try it out on a few data points from the validation set. Note that the model only works on text up to 512 characters, so we have to cut off our text after some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa33b8b",
   "metadata": {
    "id": "efa33b8b"
   },
   "outputs": [],
   "source": [
    "for text, target  in val_ds:\n",
    "    for i in range(min(3,len(text))):\n",
    "        print(\"----- Example\", i, \"-----\")\n",
    "        print(\"Sentiment classification by DistilBert:\", sentiment_pipeline(text[i].numpy()[:512].decode(\"utf-8\")))\n",
    "        print(\"Actual label:\", target[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cue5CZdhWKEN",
   "metadata": {
    "id": "cue5CZdhWKEN"
   },
   "source": [
    "Finally, let's do some systematic testing (note that we will only look at one batch, since this will take too much time otherwise. But the accuracy is relatively representative).\n",
    "\n",
    "**Discussion**: What do you observe about the performance of this pre-trained model? What do you think are the causes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ohpep96pWNUl",
   "metadata": {
    "id": "Ohpep96pWNUl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def count_ys(indiv_input,indiv_target):\n",
    "    pred = sentiment_pipeline(indiv_input.numpy()[:512].decode(\"utf-8\"))[0]['label']\n",
    "    if pred == \"POSITIVE\":\n",
    "        return 1, indiv_target\n",
    "    else:\n",
    "        return 0, indiv_target\n",
    "\n",
    "ypred = []\n",
    "ytrue = []\n",
    "for inputs, targets  in test_ds:\n",
    "    res = list(map(list, zip(*[count_ys(inputs[i],targets[i]) for i in range(len(inputs))])))\n",
    "    ypred += res[0]\n",
    "    ytrue += res[1]\n",
    "    break\n",
    "    \n",
    "print(f\"Test acc: {accuracy_score(ytrue,ypred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e2010",
   "metadata": {
    "id": "607e2010"
   },
   "source": [
    "## 3.2 Finetuning the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ndnXbYJ7P",
   "metadata": {
    "id": "d59ndnXbYJ7P"
   },
   "source": [
    "We can also fine-tune the models from ðŸ¤— Hugging Face to be more relevant for your use cases. There is a ðŸ¤— Hugging Face specific training routing that you can use, but we will try to keep things simple and instead train directly in TensorFlow, as we are used to.\n",
    "\n",
    "In either case, however, we will have to recreate our dataset. This is because each ðŸ¤— Hugging Face model expects a specific tokenization process, and it is much easier to go through this from the raw data than from data that has already been loaded into a `tf.data.Dataset`. So let's load our data from the drive into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f3430",
   "metadata": {
    "id": "201f3430"
   },
   "outputs": [],
   "source": [
    "def read_imdb_manual(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = read_imdb_manual('aclImdb/train')\n",
    "val_texts, val_labels = read_imdb_manual('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_manual('aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I1vNEzckYwjP",
   "metadata": {
    "id": "I1vNEzckYwjP"
   },
   "source": [
    "Next, we load the `Tokenizer` that is specific to the model we are using (a fine-tuned version of `DistilBert`. It is very important that you check the ðŸ¤— Hugging Face documentation before working on any models, so as to make sure you are using the right tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea28ba9",
   "metadata": {
    "id": "bea28ba9"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t-RpIFZwZBlC",
   "metadata": {
    "id": "t-RpIFZwZBlC"
   },
   "source": [
    "Now that we have our tokenizer, we will create new datasets with the data appropriately tokenized (and truncated). Note that this might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a8cb8",
   "metadata": {
    "id": "df5a8cb8"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds_new = tf.data.Dataset.from_tensor_slices((dict(tokenizer(train_texts, truncation=True, padding=True)), train_labels)).batch(batch_size)\n",
    "val_ds_new = tf.data.Dataset.from_tensor_slices((dict(tokenizer(val_texts, truncation=True, padding=True)), val_labels)).batch(batch_size)\n",
    "test_ds_new = tf.data.Dataset.from_tensor_slices((dict(tokenizer(test_texts, truncation=True, padding=True)), test_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eVpMPL2rbEiw",
   "metadata": {
    "id": "eVpMPL2rbEiw"
   },
   "source": [
    "Next, we load the actual pre-trained model and summarize it.\n",
    "\n",
    "**Discussion**: What possible issue do you notice with this? How can we fix the issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899ce6f",
   "metadata": {
    "id": "e899ce6f"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ad3FJx9qcIFM",
   "metadata": {
    "id": "Ad3FJx9qcIFM"
   },
   "source": [
    "Let's now freeze all the layers except the final classifier layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6468927",
   "metadata": {
    "id": "d6468927"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[:-2]:\n",
    "    layer.trainable=False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jw7QfOyMcYIc",
   "metadata": {
    "id": "Jw7QfOyMcYIc"
   },
   "source": [
    "We proceed to fine-tune the model. Given the slow training, we will only do so for one epoch (which already leads to quite the improvement). After fine-tuning, the test accuracy is 0.897\n",
    "\n",
    "**Discussion**: How does this compare to the previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a92a71",
   "metadata": {
    "id": "b5a92a71"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(train_ds_new,\n",
    "          validation_data=val_ds_new,\n",
    "          epochs=1)\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(test_ds_new)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d794a",
   "metadata": {
    "id": "269d794a"
   },
   "source": [
    "## 3.3 Other default model examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AXRVlTv_dk67",
   "metadata": {
    "id": "AXRVlTv_dk67"
   },
   "source": [
    "Let's take a look at two of the (many) other NLP algorithms that you can implement with the help of ðŸ¤— Hugging Face. We will focus on the pipelines here and not do any fine-tuning. But the process is the same as before if you want to go further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a7eec",
   "metadata": {
    "id": "4d3a7eec"
   },
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da1fb5",
   "metadata": {
    "id": "43da1fb5"
   },
   "source": [
    "We will first implement a Transformer-based translator (basically, our own Google Translate). All we need is the `'translation_XX_to_YY'` pipeline. For demonstration, we use `'en_to_de'`. As before, you can choose a (fitting) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15317c6",
   "metadata": {
    "id": "e15317c6"
   },
   "outputs": [],
   "source": [
    "en_de_translator = pipeline(\"translation_en_to_de\",model='t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03937a2f",
   "metadata": {
    "id": "03937a2f"
   },
   "outputs": [],
   "source": [
    "en_de_translator(\"What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04682374",
   "metadata": {
    "id": "04682374"
   },
   "source": [
    "Let's make things more complicated and use the complex translation example from the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff227b3",
   "metadata": {
    "id": "2ff227b3"
   },
   "outputs": [],
   "source": [
    "en_de_translator(\"Students start in September, have three terms with classes, finish a project, and successfully complete their degree the following summer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a166d8",
   "metadata": {
    "id": "52a166d8"
   },
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec19db",
   "metadata": {
    "id": "0eec19db"
   },
   "source": [
    "Another pipeline is the `'question-answering'` pipeline. It takes a question and a context and extracts the answer to the question from the context. The loading process should be familiar by now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fa9f9",
   "metadata": {
    "id": "915fa9f9"
   },
   "outputs": [],
   "source": [
    "qa_model = pipeline(\"question-answering\",model='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vWTxUUL_d_3P",
   "metadata": {
    "id": "vWTxUUL_d_3P"
   },
   "source": [
    "Try it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaac9f2",
   "metadata": {
    "id": "adaac9f2"
   },
   "outputs": [],
   "source": [
    "context = \"My name is Philippe and I live in London.\"\n",
    "question = \"Where do I live?\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wBF0ZHTfhD5",
   "metadata": {
    "id": "8wBF0ZHTfhD5"
   },
   "source": [
    "### Other pipelines\n",
    "\n",
    "You can find a list of the most commonly used pipelines in the [official documentation](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
    "\n",
    "**Coding**: Can you find a pipeline that allows you to implement a simple chatbot? What's the most fun conversation you can come up with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W3zd_eXOgMlZ",
   "metadata": {
    "id": "W3zd_eXOgMlZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AHmqhdRJi_pk",
   "metadata": {
    "id": "AHmqhdRJi_pk"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oDMVeuMhicE2",
   "metadata": {
    "id": "oDMVeuMhicE2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dd2f4",
   "metadata": {
    "id": "623dd2f4"
   },
   "source": [
    "# 4. Content generation with RNNs: Creating chorales like Johann Sebastian Bach (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c10940",
   "metadata": {
    "id": "41c10940"
   },
   "source": [
    "## 4.1 Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9c85e",
   "metadata": {
    "id": "32b9c85e"
   },
   "source": [
    "We will work with the Bach chorales dataset. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains a chord, made up of 4 integers, where each integer corresponds to a note's index on a piano (except for the value 0, which means that no note is played).\n",
    "\n",
    "Our objective is to train a model that can predict the next time step (consisting of four notes), given a sequence of time steps from a chorale. We will later take a look at generating Bach-like music, one note at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f631ac",
   "metadata": {
    "id": "10f631ac"
   },
   "source": [
    "First, load the training, validation, and test data from Dropbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c908a2e",
   "metadata": {
    "id": "4c908a2e"
   },
   "outputs": [],
   "source": [
    "path_to_data = tf.keras.utils.get_file(\"jsb_chorales.tgz\",\n",
    "                                       \"https://www.dropbox.com/s/9pwoax3cuylmzht/jsb_chorales.tgz?dl=1\",\n",
    "                                      extract=True)\n",
    "jsb_chorales_dir = os.path.join(os.path.abspath(os.path.join(path_to_data, os.pardir)),'jsb_chorales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00361f46",
   "metadata": {
    "id": "00361f46"
   },
   "outputs": [],
   "source": [
    "def load_chorales(path_add):\n",
    "    filepath = os.path.join(jsb_chorales_dir, path_add)\n",
    "    return_list = []\n",
    "    for file in [f for f in os.listdir(filepath) if f.endswith('.csv')]:\n",
    "        return_list.append(pd.read_csv(os.path.join(filepath, file)).values.tolist())\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0f76d",
   "metadata": {
    "id": "93d0f76d"
   },
   "outputs": [],
   "source": [
    "train_chorales = load_chorales('train')\n",
    "valid_chorales = load_chorales('valid')\n",
    "test_chorales = load_chorales('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c78fee",
   "metadata": {
    "id": "d7c78fee"
   },
   "source": [
    "Let's take a look at one data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89dbf5",
   "metadata": {
    "id": "be89dbf5"
   },
   "outputs": [],
   "source": [
    "train_chorales[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415103f",
   "metadata": {
    "id": "2415103f"
   },
   "source": [
    "We now write a few functions to listen to these chorales (you don't need to understand the details here, this is only to have a bit of fun with the exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df038bea",
   "metadata": {
    "id": "df038bea"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def notes_to_frequencies(notes):\n",
    "    # Frequency doubles when you go up one octave; there are 12 semi-tones\n",
    "    # per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n",
    "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
    "\n",
    "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
    "    note_duration = 60 / tempo # the tempo is measured in beats per minutes\n",
    "    # To reduce click sound at every beat, we round the frequencies to try to\n",
    "    # get the samples close to zero at the end of each note.\n",
    "    frequencies = np.round(note_duration * frequencies) / note_duration\n",
    "    n_samples = int(note_duration * sample_rate)\n",
    "    time = np.linspace(0, note_duration, n_samples)\n",
    "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
    "    # Removing all notes with frequencies â‰¤ 9 Hz (includes note 0 = silence)\n",
    "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
    "    return sine_waves.reshape(-1)\n",
    "\n",
    "def chords_to_samples(chords, tempo, sample_rate):\n",
    "    freqs = notes_to_frequencies(chords)\n",
    "    freqs = np.r_[freqs, freqs[-1:]] # make last note a bit longer\n",
    "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
    "                     for melody in freqs.T], axis=0)\n",
    "    n_fade_out_samples = sample_rate * 60 // tempo # fade out last note\n",
    "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
    "    merged[-n_fade_out_samples:] *= fade_out\n",
    "    return merged\n",
    "\n",
    "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100):\n",
    "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
    "    return display(Audio(samples, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4ab50",
   "metadata": {
    "id": "69c4ab50"
   },
   "source": [
    "We can use our functions to listen to the chorales. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5192ef",
   "metadata": {
    "id": "5c5192ef"
   },
   "outputs": [],
   "source": [
    "play_chords(train_chorales[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ea7f5",
   "metadata": {
    "id": "2b2ea7f5"
   },
   "source": [
    "We need to define a few things relating to the notes in the chorales. This will make pre-processing a lot easier later. Notes range from 36 (C1 = C on octave 1) to 81 (A5 = A on octave 5), plus 0 for silence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9f79c",
   "metadata": {
    "id": "35c9f79c"
   },
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48314cc6",
   "metadata": {
    "id": "48314cc6"
   },
   "source": [
    "## 4.2 Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edfc32",
   "metadata": {
    "id": "98edfc32"
   },
   "source": [
    "In order to be able to generate new chorales, we want to train a model that can predict the next chord given all the previous chords. If we naively try to predict the next chord in one shot, predicting all 4 notes at once, we run the risk of getting notes that don't go very well together. It's much better to predict one note at a time.\n",
    "\n",
    "Hence, we will need to preprocess every chorale, turning each chord into an arpeggio (i.e., a sequence of notes rather than notes played simultaneuously). So each chorale will be a long sequence of notes (rather than chords), and we can just train a model that can predict the next note given all the previous notes.\n",
    "\n",
    "So what will our input sequences be for our RNN? We will create sequences made from 32 chords (128 notes). Each x will be a sequence of 127 notes (so the notes 1-127 of the 128 notes). The corresponding y will be the same sequence, but shifted by one (so the notes 2-128 of the 128 notes). This is a \"sequence-to-sequence\" approach. You could, of course, also try a \"sequence-to-vector\" approach and only predict the last note of each sequence. However, this won't pick up very well on the combinations of nodes in chords, so I wouldn't recommend it.\n",
    "\n",
    "In principle, we could create a \"sliding window\" of 32 chords and go chord-by-chord. For example, if we have 100 chords in a chorale, then we could turn that chorale into 100 - 32 + 1 = 69 sequences of 32 chords. However, there will be a lot of correlation between the sequences, so we will instead slide our window by 16 chords. Hence, our chorale of 100 chords would now be turned into 5 sequences (the last one being from chords 65 to 97).\n",
    "\n",
    "We will use a TensorFlow `Dataset` to manage our data pre-processing. It's not strictly necessary, as the amount of data isn't massive, but it will hopefully help introduce the concepts better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054668d2",
   "metadata": {
    "id": "054668d2"
   },
   "source": [
    "We will start with a simple example before putting everything into a function. First, we convert one of our chorales datasets into a Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09594b",
   "metadata": {
    "id": "5a09594b"
   },
   "outputs": [],
   "source": [
    "chorales = tf.ragged.constant(train_chorales, ragged_rank=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1b9ca",
   "metadata": {
    "id": "0ae1b9ca"
   },
   "source": [
    "We will also shift the values of the notes so that they range from 0 to 46, where 0 represents silence, and values 1 to 46 represent notes 36 (C1) to 81 (A5). This is useful if you want to predict the notes through regression rather than classification (I don't recommend this, for reasons that will become clearer below). But it also helps to keep things more organized.\n",
    "\n",
    "**Code**: In the code below, can you complete the second line by specifying a note should remain itself if it is zero, but if it is not zero, it should be replaced by `note - min_note + 1`. Use [`tf.where`](https://www.tensorflow.org/api_docs/python/tf/where)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b243c",
   "metadata": {
    "id": "e37b243c"
   },
   "outputs": [],
   "source": [
    "dataset_all = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032a30a",
   "metadata": {
    "id": "b032a30a"
   },
   "source": [
    "Let's take a look what kind of data we have so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205fd6a",
   "metadata": {
    "id": "5205fd6a"
   },
   "outputs": [],
   "source": [
    "for item in dataset_all.take(3):\n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f55317",
   "metadata": {
    "id": "e9f55317"
   },
   "source": [
    "Each datapoint of our `Dataset` is an individual chorale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd6315",
   "metadata": {
    "id": "23cd6315"
   },
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c724ac1",
   "metadata": {
    "id": "3c724ac1"
   },
   "source": [
    "We will now turn to processing a single of those chorales. Start by creating a new `Dataset` from the chorale. Each entry of that dataset is now a chord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460c311",
   "metadata": {
    "id": "9460c311"
   },
   "outputs": [],
   "source": [
    "chorale = iter(dataset_all).next()\n",
    "dataset_single = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "for item in dataset_single.take(5):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a02fc",
   "metadata": {
    "id": "c08a02fc"
   },
   "source": [
    "To demonstrate how things work, we will look at 3 chords at a time, and slide our window chord-by-chord.\n",
    "Convert your chorale into multiple sequences of 3 chords, one apart, using the `window(3,1,drop_remainder=True)` operation on `dataset_single`. Run the complete code and compare the output of the previous step with the output below. Can you see what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d806a01",
   "metadata": {
    "id": "6d806a01"
   },
   "outputs": [],
   "source": [
    "dataset_single = dataset_single.window(3, 1, drop_remainder=True)\n",
    "dataset_single = dataset_single.flat_map(lambda window: window.batch(3))\n",
    "for item in dataset_single.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff6106",
   "metadata": {
    "id": "feff6106"
   },
   "source": [
    "We turn what we just did into a function, so we can apply it to the dataset of all chorales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854901e",
   "metadata": {
    "id": "6854901e"
   },
   "outputs": [],
   "source": [
    "def to_windows(chorale):\n",
    "    dataset_single = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "    dataset_single = dataset_single.window(3, 1, drop_remainder=True)\n",
    "    dataset_single = dataset_single.flat_map(lambda window: window.batch(3))\n",
    "    return dataset_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b1dfc",
   "metadata": {
    "id": "d29b1dfc"
   },
   "outputs": [],
   "source": [
    "dataset_all = dataset_all.flat_map(to_windows)\n",
    "for item in dataset_all.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67421482",
   "metadata": {
    "id": "67421482"
   },
   "source": [
    "Now that we have created sequences of chords to be used as inputs, we need to turn the chords into arpeggios (i.e., convert everything into long lists of notes). Here, the lists will be of length 12, because we took 3 chords at a time. However, we will later work with 32 chords, so lists of length 128.\n",
    "\n",
    "**Code**: Complete the lambda function, by applying `tf.reshape(window, [-1])` to all `window` elements in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ae388",
   "metadata": {
    "id": "174ae388"
   },
   "outputs": [],
   "source": [
    "dataset_all = dataset_all.map(lambda # \n",
    "for item in dataset_all.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87eb78",
   "metadata": {
    "id": "ae87eb78"
   },
   "source": [
    "We will also generate our `x` and our `y` from the lists:  elements 1-11 go into `x`, elements 2-12 go into `y`. Keep in mind that at runtime, our algorithm only sees **future** notes whenever it is working on one specific instance.\n",
    "\n",
    "**Code**: Again, compare the output of the previous cell with the output of the next cell to verify what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a13f44",
   "metadata": {
    "id": "74a13f44"
   },
   "outputs": [],
   "source": [
    "dataset_all = dataset_all.map(lambda #\n",
    "for item in dataset_all.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e7b8b",
   "metadata": {
    "id": "676e7b8b"
   },
   "source": [
    "We can now put everything together into a `create_dataset` function, which we apply to all the chorales, whether training, validation, or testing. There are a few additional points that come up here that we didn't discuss before, but they are commented accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EvQi7fs-Lecw",
   "metadata": {
    "id": "EvQi7fs-Lecw"
   },
   "outputs": [],
   "source": [
    "def create_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                    window_size=32, window_shift=16, cache=True):\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset_single = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset_single = dataset_single.window(window_size, window_shift, drop_remainder=True)\n",
    "        dataset_single = dataset_single.flat_map(lambda window: window.batch(window_size))\n",
    "        return dataset_single\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.map(lambda note: tf.where(note == 0, note, note - min_note + 1))\n",
    "    dataset = dataset.flat_map(to_windows)\n",
    "    dataset = dataset.map(lambda window: tf.reshape(window, [-1]))\n",
    "    if cache:\n",
    "        dataset = dataset.cache() # If the memory is sufficient, we can cache the data, which makes it faster to call up\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size) # For training, we usually want to shuffle the data randomly\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda batch: (batch[:,:-1], batch[:,1:])) # Note: we are reshaping entire batches, not just single observations\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cf973",
   "metadata": {
    "id": "3f7cf973"
   },
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_chorales, shuffle_buffer_size = 1000)\n",
    "valid_dataset = create_dataset(valid_chorales)\n",
    "test_dataset = create_dataset(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9144c3",
   "metadata": {
    "id": "3c9144c3"
   },
   "source": [
    "Let's take a look at one element from the training set to see if we have the expeted shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0OF9gechMZr5",
   "metadata": {
    "id": "0OF9gechMZr5"
   },
   "outputs": [],
   "source": [
    "temp = iter(train_dataset).next()\n",
    "print(temp[0].shape)\n",
    "print(temp[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4e6ae",
   "metadata": {
    "id": "61a4e6ae"
   },
   "source": [
    "## 4.3 Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1dd882",
   "metadata": {
    "id": "2e1dd882"
   },
   "source": [
    "We are now ready to create a model.\n",
    "\n",
    "- We could feed the note values directly to the model, as floats, but this would probably not give good results. Indeed, the relationships between notes are not that simple: for example, if you replace a C3 with a C4, the melody will still sound fine, even though these notes are 12 semi-tones apart (i.e., one octave). Conversely, if you replace a C3 with a C\\#3, it's very likely that the chord will sound horrible, despite these notes being just next to each other. So we will use an `Embedding` layer to convert each note to a small vector representation. We will use 5-dimensional embeddings, so the output of this first layer will have a shape of `[batch_size, window_size, 5]`. Keep in mind, the number of dimensions is a hyperparameter that you may want to tune\n",
    "- We will then feed this data into a stack of 4 `Conv1D` layers with. The `dilation_rate` specifies how spread apart each neuron's inputs are. The doubling of rates at each layer means that we are building a hierarchy of sequences, where the first layer captures only short sequences, and the last one captures long ones that consist of combinations of those short ones. The `'causal'` padding ensures that convolutions are padded such that there is no \"looking into the future\".\n",
    "- We intersperse the layers with `BatchNormalization` layers for faster convergence.\n",
    "- At the end, we have one `LSTM` layer to try to capture long-term patterns.\n",
    "- Finally, a `Dense` layer with `'softmax'` activation is used to produce the final note probabilities. It will predict one probability for each chorale in the batch, for each time step, and for each possible note (including silence). So the output shape will be `[batch_size, window_size, 47]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8d980",
   "metadata": {
    "id": "25a8d980"
   },
   "outputs": [],
   "source": [
    "n_embedding_dims = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(input_dim=n_notes, output_dim=n_embedding_dims,input_shape=[None]),\n",
    "    Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "    BatchNormalization(),\n",
    "    Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "    BatchNormalization(),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dense(n_notes, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1b23a",
   "metadata": {
    "id": "ffc1b23a"
   },
   "source": [
    "The function below trains the model and plots the progress. Be aware that training may take a little bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc92f3d",
   "metadata": {
    "id": "2bc92f3d"
   },
   "outputs": [],
   "source": [
    "def train_and_plot(model, learning_rate = 0.001, epochs = 20):\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "                    metrics = ['accuracy'])\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience = 10, restore_best_weights=True)\n",
    "    log = model.fit(train_dataset, epochs=epochs,\n",
    "                        validation_data=valid_dataset,\n",
    "                    callbacks = [early_stopping_cb])\n",
    "    \n",
    "    plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "    plt.plot(log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "    plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "    plt.plot(log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "    plt.legend()\n",
    "    ax = plt.gca()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai2DoapnTynr",
   "metadata": {
    "id": "ai2DoapnTynr"
   },
   "outputs": [],
   "source": [
    "train_and_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732829d",
   "metadata": {
    "id": "4732829d"
   },
   "source": [
    "## 4.4 Creating our own chorale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa39654b",
   "metadata": {
    "id": "aa39654b"
   },
   "source": [
    "Now that we can predict the next note of a chorale, let's create some music! We will take a few starting chords, predict the next note to play, then take all the notes so far (the starting ones and the predicted one), and predict the next note, and so on. To do this, we need to process our data a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bd706",
   "metadata": {
    "id": "e66bd706"
   },
   "source": [
    "**Code**: Let's start somewhere. We will use 8 intial chords to start our own chorale. For simplicity, we take the first 8 chords from a test set chorale.\n",
    "You will need to convert the notes back (from 36-81 to 1-46), unless a note is already a zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d85c8",
   "metadata": {
    "id": "ba6d85c8"
   },
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[0][:8]\n",
    "arpeggio = #\n",
    "arpeggio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85b06b",
   "metadata": {
    "id": "9e85b06b"
   },
   "source": [
    "Keep in mind that we made predictions on lists of notes (arpeggios) instead of chords. Hence, we convert our starting chords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f9e7a",
   "metadata": {
    "id": "258f9e7a"
   },
   "outputs": [],
   "source": [
    "arpeggio = tf.reshape(arpeggio, [1,-1])\n",
    "arpeggio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10906496",
   "metadata": {
    "id": "10906496"
   },
   "source": [
    "**Code**: Then, we predict the most likely note to come next. Use `model.predict` as well as `np.argmax` (with `axis=-1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c580f2",
   "metadata": {
    "id": "24c580f2"
   },
   "outputs": [],
   "source": [
    "next_note = #\n",
    "next_note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33afa9a",
   "metadata": {
    "id": "a33afa9a"
   },
   "source": [
    "The code above predicts as many notes as we gave as an input (because of the sequence-to-sequence model structure). We only want the last chord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2273de",
   "metadata": {
    "id": "db2273de"
   },
   "outputs": [],
   "source": [
    "next_note = next_note[:1,-1:]\n",
    "next_note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff505bd",
   "metadata": {
    "id": "0ff505bd"
   },
   "source": [
    "We can now add this note to our arpeggio, so we can keep predicting further notes.\n",
    "\n",
    "**Code**: Use `tf.concat` with `axis=1`. You can concatenate the `arpeggio` with the `next_note` by putting them inside a list which is the primary input to `tf.concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0401b98",
   "metadata": {
    "id": "a0401b98"
   },
   "outputs": [],
   "source": [
    "arpeggio = #\n",
    "arpeggio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7d3ae",
   "metadata": {
    "id": "b2f7d3ae"
   },
   "source": [
    "Let's wrap all this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba459cf",
   "metadata": {
    "id": "1ba459cf"
   },
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, output_chords):\n",
    "    arpeggio = tf.constant([[note if note == 0 else note - min_note + 1 for note in chord] for chord in seed_chords], dtype=tf.int64)\n",
    "    arpeggio = tf.reshape(arpeggio, [1,-1])\n",
    "    for chord in range(output_chords):\n",
    "        for note in range(4):\n",
    "            next_note = np.argmax(model.predict(arpeggio), axis=-1)[:1,-1:]\n",
    "            arpeggio = tf.concat([arpeggio, next_note], axis=1)\n",
    "    arpeggio = tf.where(arpeggio == 0, arpeggio, arpeggio + min_note - 1)\n",
    "    new_chorale = tf.reshape(arpeggio, shape=[-1, 4])\n",
    "    return new_chorale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363663f2",
   "metadata": {
    "id": "363663f2"
   },
   "source": [
    "Try it out by creating your very own chorale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1fe6f",
   "metadata": {
    "id": "0fd1fe6f"
   },
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale(model, seed_chords, 56)\n",
    "play_chords(new_chorale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e33b51",
   "metadata": {
    "id": "80e33b51"
   },
   "source": [
    "If you listened to your chorale, you may have noticed a major flaw of the approach: it is often too conservative. The model will not take any risk, it will always choose the note with the highest probability, and since repeating the previous note generally sounds good enough, it's the least risky option, so the algorithm will tend to make notes last longer and longer (and, if you are unlucky, simply play one chord after just a few time steps). Pretty boring. Plus, if you run the model multiple times, it will always generate the same melody.\n",
    "\n",
    "So let's spice things up a bit! Instead of always picking the note with the highest score, we will pick the next note randomly, according to the predicted probabilities. For example, if the model predicts a C3 with 75% probability, and a G3 with a 25% probability, then we will pick one of these two notes randomly, with these probabilities. We will also add a temperature parameter that will control how \"hot\" (i.e., daring) we want the system to feel. A high temperature will bring the predicted probabilities closer together, reducing the probability of the likely notes and increasing the probability of the unlikely ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef2f0d",
   "metadata": {
    "id": "1fef2f0d"
   },
   "outputs": [],
   "source": [
    "def generate_chorale_v2(model, seed_chords, output_chords, temperature=1):\n",
    "    arpeggio = tf.constant([[note if note == 0 else note - min_note + 1 for note in chord] for chord in seed_chords], dtype=tf.int64)\n",
    "    arpeggio = tf.reshape(arpeggio, [1,-1])\n",
    "    for chord in range(output_chords):\n",
    "        for note in range(4):\n",
    "            next_note_probas = model.predict(arpeggio)[0, -1:]\n",
    "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
    "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
    "            arpeggio = tf.concat([arpeggio, next_note], axis=1)\n",
    "    arpeggio = tf.where(arpeggio == 0, arpeggio, arpeggio + min_note - 1)\n",
    "    new_chorale = tf.reshape(arpeggio, shape=[-1, 4])\n",
    "    return new_chorale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d6c9c",
   "metadata": {
    "id": "3e0d6c9c"
   },
   "source": [
    "Try it out again. This should sound much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de53955",
   "metadata": {
    "id": "9de53955"
   },
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale_v2(model, seed_chords, 56)\n",
    "play_chords(new_chorale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c283d",
   "metadata": {
    "id": "368c283d"
   },
   "source": [
    "# 5. Text generation with large language models and the risks of bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XkGASFGKjV63",
   "metadata": {
    "id": "XkGASFGKjV63"
   },
   "source": [
    "We will try out the `'text-generation'` pipeline, using GPT2 (the predecessor of GPT3, which powers ChatGPT, but is not available publicly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe654e",
   "metadata": {
    "id": "cbbe654e"
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xeg0SW7Njkb9",
   "metadata": {
    "id": "xeg0SW7Njkb9"
   },
   "source": [
    "With the text-generation funcionality, you give a prompt, and the model completes the sentence. Here, we are only looking at 20 words returned at maximum.\n",
    "\n",
    "**Discussion**: In the example below, what do you notice? What do you think the causes are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb99b9",
   "metadata": {
    "id": "89bb99b9"
   },
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(99)\n",
    "generator(\"The man worked as a\", max_length=20, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e38ee",
   "metadata": {
    "id": "2d1e38ee"
   },
   "outputs": [],
   "source": [
    "set_seed(99)\n",
    "generator(\"The woman worked as a\", max_length=20, num_return_sequences=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
