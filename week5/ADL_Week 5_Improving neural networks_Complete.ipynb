{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd1bc41",
   "metadata": {},
   "source": [
    "# Implementing advanced procedures and algorithms in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdc8d6",
   "metadata": {},
   "source": [
    "This notebook collects advanced procedures relevant to training more complex neural networks. It gives you the possibility to look up the procedures as needed and copy the relevant code. The notebook builds on the books by Aurélien Géron and François Chollet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31838b2",
   "metadata": {},
   "source": [
    "# 1. From the previous notebook: A multiclass-classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c824b11",
   "metadata": {},
   "source": [
    "Throughout, we will train a neural network on a dataset of fashion-products that is labeled with the categories of each product. The data is loaded directly from TensorFlow (which has quite the broad collection of datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_other, y_other), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_other.shape)\n",
    "print(y_other.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502fd8f",
   "metadata": {},
   "source": [
    "The X's are matrices (with 28x28 pixels), while the y's are numbers.\n",
    "Let's plot two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_test[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_test[500],cmap=\"binary\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce05e70f",
   "metadata": {},
   "source": [
    "As well as the corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[0])\n",
    "print(y_test[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4802b77",
   "metadata": {},
   "source": [
    "That's a bit hard to interpret. Luckily, we have the right names for each label available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13cd27",
   "metadata": {},
   "source": [
    "We can now take another look at what the pictures above represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[y_test[0]])\n",
    "print(class_names[y_test[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37aff5",
   "metadata": {},
   "source": [
    "Finally, we divide the values of X by 255 (essentially standardizing the pixel-values to 0-1) and also split apart a validation set (of the same size as the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09935b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other = X_other / 255.\n",
    "X_test = X_test / 255.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_other, y_other, train_size = 50000, random_state=152)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473138d8",
   "metadata": {},
   "source": [
    "# 2. Help! My model won't train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ebe4f",
   "metadata": {},
   "source": [
    "Sometimes models just won't train, or they only achieve very bad performance. Don't worry, luckily we can always train *some* model, because we can even learn from random data (now, whether this is desirable is another question). Let's see what could be the issue with training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaabf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e72fb",
   "metadata": {},
   "source": [
    "Use the `summary` function, to see whether everything worked out as it should. If we defined the model as discussed above, we should get a total of 407,050 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25373f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91d79b",
   "metadata": {},
   "source": [
    "We can now compile the model. We use `optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=1.0)` and `metrics=['accuracy']`. For the loss, we use `sparse_categorical_crossentropy`. This is because our y's here are **not** one-hot-encoded, but instead are values from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d71356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=1.0),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9e0f4",
   "metadata": {},
   "source": [
    "Train the model for 15 epochs, keeping track also of the `validation_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee79f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=10, batch_size = 128,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175357c",
   "metadata": {},
   "source": [
    "Take a look at the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(log):\n",
    "    plt.plot(log.history['accuracy'],label = \"training accuracy\",color='green')\n",
    "    plt.plot(log.history['loss'],label = \"training loss\",color='darkgreen')\n",
    "    plt.plot(log.history['val_accuracy'], label = \"validation accuracy\",color='grey')\n",
    "    plt.plot(log.history['val_loss'], label = \"validation loss\",color='darkblue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8883a32",
   "metadata": {},
   "source": [
    "Of course, we are not making great predictions with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:4]\n",
    "y_predict = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_predict = [class_names[y] for y in y_predict]\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X_test[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_test[1],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(X_test[2],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(X_test[3],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "print(\"Predictions are: \" + str(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d38f9b",
   "metadata": {},
   "source": [
    "Let's try this again, by lowering our learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=10, batch_size = 128,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f747a",
   "metadata": {},
   "source": [
    "Looking much better, right? Our predictions also make somewhat more sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06013504",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:4]\n",
    "y_predict = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_predict = [class_names[y] for y in y_predict]\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X_test[0],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_test[1],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(X_test[2],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(X_test[3],cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "print(\"Predictions are: \" + str(y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf4e93",
   "metadata": {},
   "source": [
    "## 2.1 Other changes that may be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa7d56",
   "metadata": {},
   "source": [
    "### Defining the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e8f6b",
   "metadata": {},
   "source": [
    "You might not have noticed, but we run mini-batch gradient descent by default. We can control the batch-size within the `model.fit` function. The default is `32`. Run the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=2,batch_size=8,\n",
    "                validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b4fd8",
   "metadata": {},
   "source": [
    "Can you remake the model, but change the `batch_size` to `1024`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc02406",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=30, batch_size=256,\n",
    "                validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c220c",
   "metadata": {},
   "source": [
    "Notice the number of steps taken in each epoch (the counter just underneath \"Epoch x/30\"). Can you explain where the number of steps are coming from? What do you notice about the time taken?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b81f19",
   "metadata": {},
   "source": [
    "### Using Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77537c1f",
   "metadata": {},
   "source": [
    "We can add momentum to many algorithms. The base case is to add momentum to `SGD`. A typical value is `0.9` but keep in mind that this is another hyperparameter that may need some tuning. When setting up `SGD`, you can also tick `nesterov=True` to use the Nesterov algorithm, a moment-based algorithm we didn't discuss. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f507ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01, momentum=0.9, nesterov=False),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,epochs=3,batch_size=128,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378536d2",
   "metadata": {},
   "source": [
    "### RMSpop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b6580",
   "metadata": {},
   "source": [
    "RMSprop is an algorithm that pursues a slightly different idea: it normalizes the gradients using their squares. It requires to specify a `learning_rate`, as well as the hyperparameters `rho` and `epsilon`. For the latter two, the standard values usually do just fine, while even the `learning_rate` is less problematic than in `SGD`.\n",
    "\n",
    "If you want, you can also add `momentum` to the algorithm.\n",
    "\n",
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87deefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-07, momentum=0.0),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,epochs=3,batch_size=128,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a480df7",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5324f9d3",
   "metadata": {},
   "source": [
    "Finally, we have `Adam`, which is used as commonly (or even more) than `RMSprop`. Adam combines the ideas of RMSprop and momentum gradient descent. However, it also adds a slight adjustment that is particularly relevant for early iterations. The hyperparameters are `learning_rate`, `beta_1`, `beta_2`, and `epislon`, even though mostly people leave everything but the `learning_rate` alone. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,epochs=3,batch_size=128,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fd868",
   "metadata": {},
   "source": [
    "### Specific initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a5e08",
   "metadata": {},
   "source": [
    "We generally want to initialize our weights in a sensible manner (especially if we are not using batch normalization, for example, because of runtime concerns). Let's start with our baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(631)\n",
    "random.seed(631)\n",
    "np.random.seed(631)\n",
    "tf.random.set_seed(631)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(\"First layer: \" + str(weights[0,0]))\n",
    "weights, biases = model.layers[-1].get_weights()\n",
    "print(\"Last layer: \" + str(weights[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eeffbb",
   "metadata": {},
   "source": [
    "Can you copy the model definition, but change the first layer to `kernel_initializer='he_normal'` and the output layer to `kernel_initializer='glorot_uniform'`?\n",
    "\n",
    "What changes do you observe in the first layer, what changes in the last layer? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(631)\n",
    "random.seed(631)\n",
    "np.random.seed(631)\n",
    "tf.random.set_seed(631)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(\"First layer: \" + str(weights[0,0]))\n",
    "weights, biases = model.layers[-1].get_weights()\n",
    "print(\"Last layer: \" + str(weights[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078eaff",
   "metadata": {},
   "source": [
    "# 3. Enabling and speeding up training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ab13f",
   "metadata": {},
   "source": [
    "Below, we create a much deeper neural network. As you can see, not much is happening in terms of learning for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=10,batch_size=128,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254a655",
   "metadata": {},
   "source": [
    "## 3.1 Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379372d8",
   "metadata": {},
   "source": [
    "Batch normalization allows us to do normalization at all stages of the network. For each input that is normalized, we need 4 parameters:\n",
    "1. One that determines how the input is scaled (trainable)\n",
    "1. One that determines how the input is shifted (trainable)\n",
    "1. One that keeps track of the average of that input (non-trainable - it is still being adjusted though!)\n",
    "1. One that keeps track of the standard deviation of that input (non-trainable - it is still being adjusted though!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8d91f",
   "metadata": {},
   "source": [
    "### Option 1: After activation (before inputs are weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63327a",
   "metadata": {},
   "source": [
    "We can simply add a `BatchNormalization` layer before each of our `Dense` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2159082",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5a59f",
   "metadata": {},
   "source": [
    "Can you verify the number of trainable and non-trainable parameters?\n",
    "\n",
    "Let's now train the network again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b55f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=5,batch_size=128,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2130c3a",
   "metadata": {},
   "source": [
    "We can train even a deep neural network much more easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838d717",
   "metadata": {},
   "source": [
    "### Option 2: Before activation (after inputs are weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765779de",
   "metadata": {},
   "source": [
    "The reommendation by the authors of the original paper on batch normalization is to normalize the weighted sum that goes into the neurons. That is, we first combine the inputs (and add a bias), then we \"normalize\" that weighted sum, before running the activation function on it. To do so in TensorFlow, we have to split apart our hidden layers into the combination and the activaiton. We out the `BatchNormalization` in-between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(30),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=5, batch_size=128,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e5c93",
   "metadata": {},
   "source": [
    "In practice, the differences between the two options tend to be small. But if you are really struggling to get your network to learn, try it out like this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b0242",
   "metadata": {},
   "source": [
    "## 3.2 Learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e8116",
   "metadata": {},
   "source": [
    "### Power scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c3a68",
   "metadata": {},
   "source": [
    "Remember that each epoch contains a number of steps ($\\frac{n}{\\text{mini-batch-size}}$ to be exact). If we want to express our decay schedule based on the number of epochs, we first have to make a bit of an adjustment. For example, say that we specify the min-batch-size to 128 and that we want to have reach the next \"decay step\" (i.e., 1/2, 1/3, 1/4, ...) after 5 epochs.\n",
    "\n",
    "Can you define the correct `s`, which should be the number of steps (not epochs!) until we reach the next \"decay step\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee339b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs_until_change = 5\n",
    "steps_per_epoch = X_train.shape[0] / batch_size\n",
    "s = epochs_until_change * steps_per_epoch\n",
    "\n",
    "print(\"epochs_until_change:\",epochs_until_change)\n",
    "print(\"steps_per_epoch:\",steps_per_epoch)\n",
    "print(\"s:\",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180cb4c",
   "metadata": {},
   "source": [
    "Once we have defined the right `s`, we can train the model by manually defining our optimizer, using the TensorFlow scheduling process.\n",
    "\n",
    "Here, we use `InverseTimeDecay` which computes\n",
    "```\n",
    "current_learning_rate = initial_learning_rate / (1 + decay_rate * step / decay_steps)\n",
    "```\n",
    "Increasing the `decay_rate` is equivalent to decreasing the `decay_steps`. Since we have already tuned `decay_steps=s`, we can simply set the `decay_rate` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd998af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=0.01, decay_steps=s, decay_rate=1)\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=10,batch_size=batch_size,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6ab1a",
   "metadata": {},
   "source": [
    "Let's compare this to the case with just a flat `learning_rate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(learning_rate = 0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=10,batch_size=batch_size,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358988d",
   "metadata": {},
   "source": [
    "### Exponential scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32798f00",
   "metadata": {},
   "source": [
    "We now use the `ExponentialDecay` schedule. Here, the computation is\n",
    "```\n",
    "current_learning_rate = initial_learning_rate * decay_rate**(step / decay_steps)\n",
    "```\n",
    "(Note that `**` means to the power of)\n",
    "\n",
    "Our baseline exponential schedule has a `0.1` base, so we set `decay_rate=0.1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=s, decay_rate=0.1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=10,batch_size=batch_size,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2238527",
   "metadata": {},
   "source": [
    "# 4. A lack of overfitting - increasing the capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=20,batch_size=128,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train,\n",
    "                epochs=20,batch_size=128,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f04788",
   "metadata": {},
   "source": [
    "# 5. Reducing bias through hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66596f96",
   "metadata": {},
   "source": [
    "One option to run HP tuning relies on TensorBoard and makes things quite visual. It's also relatively intuitive, but doesn't have as much functionality as the Keras Tuner, which we will see in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d2510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c65d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3fd77",
   "metadata": {},
   "source": [
    "The following command deletes the log folder and may be useful for cleaning up. But be careful not to delete the things you still want to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5934a",
   "metadata": {},
   "source": [
    "We start by defining the parameters to tune over. We will tune the learning rate, the choice of optimizer, the dropout rate, and the number of hidden units per layer. We will stay fixed with two hidden layers, however.\n",
    "\n",
    "Using HParams, we define the parameters, as well as the interval over which we may vary them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001,1.0))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'rmsprop']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.3))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete(range(50,150)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51dcf1d",
   "metadata": {},
   "source": [
    "We also need to define the metrics to measure. We will only care about the accuracy in our case, since we are performing a classification on balanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141feb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_ACCURACY = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6961d55",
   "metadata": {},
   "source": [
    "Once we have set up our parameters and metrics, we write those into our folder with the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd43ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(hparams=[HP_LEARNING_RATE, HP_OPTIMIZER, HP_DROPOUT, HP_NUM_UNITS],\n",
    "                      metrics = [hp.Metric(METRIC_ACCURACY, display_name='Accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d81f1",
   "metadata": {},
   "source": [
    "Next, we define a function that creates and trains a model, and evaluates it on the test set. This function will get a dictionary `hparams`, that contains the different parameter choices. Hence, the way the model is build is kept variable.\n",
    "\n",
    "The function also logs the choice of parameters and the output of our function (the mse), in order to display both in TensorBoard. For this purpose, we give it the current directory where the relevant information should be kept, `run_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, run_dir):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
    "    \n",
    "    if hparams[HP_OPTIMIZER] == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.experimental.RMSprop(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "    elif hparams[HP_OPTIMIZER] == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "        \n",
    "    model.compile(  optimizer=optimizer,\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=5)\n",
    "    accuracy = model.evaluate(X_valid, y_valid)[1]\n",
    "    \n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bd757",
   "metadata": {},
   "source": [
    "Finally, we run a few different choices of parameters. Remember to choose parameters randomly! It is fine to sample the `dropout_rate`, `num_units` and `optimizer` at uniformly, but keep in mind the scaling issue when it comes to the `learning_rate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de85ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sessions = 3\n",
    "\n",
    "for session in range(total_sessions):\n",
    "    \n",
    "    # Create hyperparameters randomly\n",
    "    dropout_rate = HP_DROPOUT.domain.sample_uniform()\n",
    "    num_units = HP_NUM_UNITS.domain.sample_uniform()\n",
    "    optimizer = HP_OPTIMIZER.domain.sample_uniform()\n",
    "    \n",
    "    r = -3*np.random.rand()\n",
    "    learning_rate = 10.0**r\n",
    "    \n",
    "    # Create a dictionary of hyperparameters\n",
    "    hparams = { HP_LEARNING_RATE: learning_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_DROPOUT: dropout_rate,\n",
    "                HP_NUM_UNITS: num_units}\n",
    "    \n",
    "    # train the model with the chosen parameters\n",
    "    run_name = \"run-%d\" % session\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    train_test_model(hparams,'logs/hparam_tuning/' + run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd459da9",
   "metadata": {},
   "source": [
    "Finally, we display the runs using TensorBoard. If you are lucky, it is enough to call\n",
    "```\n",
    "%tensorboard --logdir logs\n",
    "```\n",
    "Otherwise, you might have to specify the path to the TensorBoard binary, such as here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6be7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TENSORBOARD_BINARY'] = '/Users/philippe/anaconda3/envs/adl_env/bin/tensorboard'\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11037455",
   "metadata": {},
   "source": [
    "Once you have chosen a model that you like based on the hyperparameter search, how do you proceed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6466b402",
   "metadata": {},
   "source": [
    "# 6. Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77681f67",
   "metadata": {},
   "source": [
    "## 6.1 Spurious correlations as a source of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ade47",
   "metadata": {},
   "source": [
    "We will see here that noise in your data makes your predictions worse - because it introduces spurious correlations that the model \"learns\" but that are actually without any meaning for generalization (i.e., that lead to more and more overfitting).\n",
    "\n",
    "To show this, let's add additional features to the data. In one case, we will add features that are all zero, and in another case we will add features that are random. Note that we flatten the data first in this case, to make this process easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43151e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = X_other.reshape(X_other.shape[0],X_other.shape[1]**2)\n",
    "X_with_noise = np.concatenate([X_flat, np.random.random((X_flat.shape[0],X_flat.shape[1]))], axis=1)\n",
    "X_with_zero = np.concatenate([X_flat, np.zeros((X_flat.shape[0],X_flat.shape[1]))],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42809bfa",
   "metadata": {},
   "source": [
    "Take a look at the shapes of our new data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab570baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_flat.shape)\n",
    "print(X_with_noise.shape)\n",
    "print(X_with_zero.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73b724",
   "metadata": {},
   "source": [
    "We will now create the same model twice. Note that we no longer need a `Flatten` layer, because we already flattened the data. Also, we only applied our transformation once, to the `X_other` data. Hence, instead of resplitting our data, we will let `TensorFlow` do it for us, using `validation_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "log_noise = model.fit(X_with_noise, y_other,\n",
    "                        epochs=5,batch_size=128,\n",
    "                        validation_split = 0.2)\n",
    "log_zero = model.fit(X_with_zero, y_other,\n",
    "                        epochs=5,batch_size=128,\n",
    "                        validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ff4e6",
   "metadata": {},
   "source": [
    "Plot the results below. You will see that adding noise is always detrimental to the generalization ability of your model. The more noise, the worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_noise.history['val_accuracy'],label = \"Noise added\",color='green')\n",
    "plt.plot(log_zero.history['val_accuracy'], label = \"Zero added\",color='darkblue')\n",
    "plt.title(\"Validation accuracies\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243b595",
   "metadata": {},
   "source": [
    "# 6.2 Lowering variance through regularization\n",
    "\n",
    "We learned about a number of regularization techniques. Here, we will see how to implement early stopping, L2-regularization, and dropout-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2aeb7",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e967ce4",
   "metadata": {},
   "source": [
    "When we implement early stopping, the model definition and compilation is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2f299",
   "metadata": {},
   "source": [
    "However, we now need to add a `callback` to the training process. We define the `EarlyStopping` callback, which interrupts training if the validation loss is no longer improving. In particular, the callback waits for `patience` epochs of no improvement before interrupting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271305c",
   "metadata": {},
   "source": [
    "The other parameter here is `restore_best_weights`. If set to `True`, this simply means that, once the callback decides to interrupt, it takes the version of the model that led to the best validation loss so far (do you know which epoch this corresponds to?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit(X_train, y_train, epochs=50,batch_size=128,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=[early_stopping_cb])\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d9086",
   "metadata": {},
   "source": [
    "### L2- regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c479a",
   "metadata": {},
   "source": [
    "L2-regularization adds a penalty based on the L2-norm to the loss function. Usually, we add the same penalty for all weights, and we don't add a penalty for the biases. But you could also add a `bias_regularizer`, or even an `activity_regularizer`, which regularizes the output of the neurons instead of the parameters.\n",
    "\n",
    "Keep in mind that the regularization parameter is another hyperparameter that might need tuning. A good starting point is 0.01, but it can vary quite a bit depending on the problem and network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param = 0.01\n",
    "regularizer = tf.keras.regularizers.l2(reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614e6d8",
   "metadata": {},
   "source": [
    "Can you rerun the model from above, but using regularization? In particular, to each `Dense` layer, you want to add the argument `kernel_regularizer=regularizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4255db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\",kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\",kernel_regularizer=regularizer)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=10,batch_size=128,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ecc21",
   "metadata": {},
   "source": [
    "As the regularization term is added when training but not when validating/testing, the loss will typically start out quite high, before the optimization routine finds a good way to adjust the weights to reduce the loss.\n",
    "\n",
    "Note that we have improved the overfitting issue quite a bit, but unfortunately we have made it more difficult for the model to learn (we introduced bias). This is frequently the case, and we usually need to do some fiddling to find a good compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97cc8d",
   "metadata": {},
   "source": [
    "### Dropout-regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef8ffa",
   "metadata": {},
   "source": [
    "Another regularization method is dropout-regularization. At training time, in each iteration, a number of neurons will be considered as non-existent, so we force the network to distribute weights more equally across neurons.\n",
    "\n",
    "This makes the correct computation for activations a bit challenging when doing predictions, but luckily TensorFlow takes care of the added complexity.\n",
    "\n",
    "If we want to ensure that neurons at a certain layer drop out (with probability `rate`), we add a `Dropout` layer before the corresponding `Dense` layer, using\n",
    "```\n",
    "tf.keras.layers.Dropout(rate=0.2)\n",
    "```\n",
    "Of course, `0.2` is just a particular choice and we can vary that.\n",
    "Can you repeat the previous (baseline) model, but adding a `Dropout` layer before each `Dense` layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "log = model.fit(X_train, y_train, epochs=10,batch_size=128,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "create_plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_env",
   "language": "python",
   "name": "adl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
